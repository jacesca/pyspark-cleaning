{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd4c136e-e407-4ec3-b0e6-a1819f93735a",
   "metadata": {},
   "source": [
    "# Improving Performance\n",
    "\n",
    "Improve data cleaning tasks by increasing performance or reducing resource requirements.\n",
    "\n",
    "## Preparing the environment\n",
    "\n",
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84770b51-2d57-4b9d-b8ff-bc5b8773783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from pyspark.sql.types import (_parse_datatype_string, StructType, StructField,\n",
    "                               DoubleType, IntegerType, StringType, FloatType)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44608dc4-fe61-4d75-a337-7c8ef9d7f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore warnings coming from Arrow optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3751f7-cc7f-4c27-9055-d76887924963",
   "metadata": {},
   "source": [
    "### Connect to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d00fd800-7416-4120-b336-c915d9797bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "                     .config(\"spark.sql.repl.eagerEval.enabled\", True)  # eval DataFrame in notebooks\n",
    "                     .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffd0fb21-d1fe-49c0-abf0-15b3702f5313",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea505934-a70b-4017-9af1-e33c8399adb0",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22b83775-1081-484a-a393-9944be747d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date (MM/DD/YYYY): date (nullable = true)\n",
      " |-- Flight Number: integer (nullable = true)\n",
      " |-- Destination Airport: string (nullable = true)\n",
      " |-- Actual elapsed time (Minutes): integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Date (MM/DD/YYYY)</th><th>Flight Number</th><th>Destination Airport</th><th>Actual elapsed time (Minutes)</th></tr>\n",
       "<tr><td>2014-01-01</td><td>5</td><td>HNL</td><td>519</td></tr>\n",
       "<tr><td>2014-01-01</td><td>7</td><td>OGG</td><td>505</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+-------------+-------------------+-----------------------------+\n",
       "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
       "+-----------------+-------------+-------------------+-----------------------------+\n",
       "|       2014-01-01|            5|                HNL|                          519|\n",
       "|       2014-01-01|            7|                OGG|                          505|\n",
       "+-----------------+-------------+-------------------+-----------------------------+"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_2014 = spark.read.csv('data-sources/AA_DFW_2014_Departures_Short.csv.gz', header=True, inferSchema=True)\n",
    "\n",
    "# cast to date\n",
    "flights_2014 = flights_2014.withColumn(\"Date (MM/DD/YYYY)\", \n",
    "                                       F.to_date(flights_2014[\"Date (MM/DD/YYYY)\"], \"MM/dd/yyyy\"))\n",
    "\n",
    "flights_2014.createOrReplaceTempView(\"flights_2014\")\n",
    "flights_2014.printSchema()\n",
    "flights_2014.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b689046-34a9-4503-8e59-97f61b064c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date (MM/DD/YYYY): date (nullable = true)\n",
      " |-- Flight Number: integer (nullable = true)\n",
      " |-- Destination Airport: string (nullable = true)\n",
      " |-- Actual elapsed time (Minutes): integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Date (MM/DD/YYYY)</th><th>Flight Number</th><th>Destination Airport</th><th>Actual elapsed time (Minutes)</th></tr>\n",
       "<tr><td>2015-01-01</td><td>5</td><td>HNL</td><td>526</td></tr>\n",
       "<tr><td>2015-01-01</td><td>7</td><td>OGG</td><td>517</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+-------------+-------------------+-----------------------------+\n",
       "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
       "+-----------------+-------------+-------------------+-----------------------------+\n",
       "|       2015-01-01|            5|                HNL|                          526|\n",
       "|       2015-01-01|            7|                OGG|                          517|\n",
       "+-----------------+-------------+-------------------+-----------------------------+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_2015 = spark.read.csv('data-sources/AA_DFW_2015_Departures_Short.csv.gz', header=True, inferSchema=True)\n",
    "\n",
    "# cast to date\n",
    "flights_2015 = flights_2015.withColumn(\"Date (MM/DD/YYYY)\", \n",
    "                                       F.to_date(flights_2015[\"Date (MM/DD/YYYY)\"], \"MM/dd/yyyy\"))\n",
    "\n",
    "flights_2015.createOrReplaceTempView(\"flights_2015\")\n",
    "flights_2015.printSchema()\n",
    "flights_2015.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96fcc16c-b1ea-4219-9833-ad050a1d25d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date (MM/DD/YYYY): date (nullable = true)\n",
      " |-- Flight Number: integer (nullable = true)\n",
      " |-- Destination Airport: string (nullable = true)\n",
      " |-- Actual elapsed time (Minutes): integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Date (MM/DD/YYYY)</th><th>Flight Number</th><th>Destination Airport</th><th>Actual elapsed time (Minutes)</th></tr>\n",
       "<tr><td>2017-01-01</td><td>5</td><td>HNL</td><td>537</td></tr>\n",
       "<tr><td>2017-01-01</td><td>7</td><td>OGG</td><td>498</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+-------------+-------------------+-----------------------------+\n",
       "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
       "+-----------------+-------------+-------------------+-----------------------------+\n",
       "|       2017-01-01|            5|                HNL|                          537|\n",
       "|       2017-01-01|            7|                OGG|                          498|\n",
       "+-----------------+-------------+-------------------+-----------------------------+"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_2017 = spark.read.csv('data-sources/AA_DFW_2017_Departures_Short.csv.gz', header=True, inferSchema=True)\n",
    "\n",
    "# cast to date\n",
    "flights_2017 = flights_2017.withColumn(\"Date (MM/DD/YYYY)\", \n",
    "                                       F.to_date(flights_2017[\"Date (MM/DD/YYYY)\"], \"MM/dd/yyyy\"))\n",
    "flights_2017.createOrReplaceTempView(\"flights_2017\")\n",
    "flights_2017.printSchema()\n",
    "flights_2017.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfbb6415-4ff7-417a-99ea-2851ce4ac093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date (MM/DD/YYYY): date (nullable = true)\n",
      " |-- Flight Number: integer (nullable = true)\n",
      " |-- Destination Airport: string (nullable = true)\n",
      " |-- Actual elapsed time (Minutes): integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Date (MM/DD/YYYY)</th><th>Flight Number</th><th>Destination Airport</th><th>Actual elapsed time (Minutes)</th></tr>\n",
       "<tr><td>2018-01-01</td><td>5</td><td>HNL</td><td>498</td></tr>\n",
       "<tr><td>2018-01-01</td><td>7</td><td>OGG</td><td>501</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+-------------+-------------------+-----------------------------+\n",
       "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
       "+-----------------+-------------+-------------------+-----------------------------+\n",
       "|       2018-01-01|            5|                HNL|                          498|\n",
       "|       2018-01-01|            7|                OGG|                          501|\n",
       "+-----------------+-------------+-------------------+-----------------------------+"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_2018 = spark.read.csv('data-sources/AA_DFW_2018_Departures_Short.csv.gz', header=True, inferSchema=True)\n",
    "\n",
    "# cast to date\n",
    "flights_2018 = flights_2018.withColumn(\"Date (MM/DD/YYYY)\", \n",
    "                                       F.to_date(flights_2018[\"Date (MM/DD/YYYY)\"], \"MM/dd/yyyy\"))\n",
    "\n",
    "# save the file in csv fprmat\n",
    "(flights_2018.repartition(5)\n",
    "             .write.format('csv')\n",
    "             .save('output-files/AA_DFW_2018_Departures_Short.csv', mode='overwrite'))\n",
    "\n",
    "# Review the data\n",
    "flights_2018.createOrReplaceTempView(\"flights_2018\")\n",
    "flights_2018.printSchema()\n",
    "flights_2018.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da08c587-0bbe-4a65-955d-f35c931ff7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DATE: date (nullable = true)\n",
      " |-- TITLE: string (nullable = true)\n",
      " |-- VOTER_NAME: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>DATE</th><th>TITLE</th><th>VOTER_NAME</th></tr>\n",
       "<tr><td>2017-02-08</td><td>Councilmember</td><td>Jennifer S. Gates</td></tr>\n",
       "<tr><td>2017-02-08</td><td>Councilmember</td><td>Philip T. Kingston</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+-------------+------------------+\n",
       "|      DATE|        TITLE|        VOTER_NAME|\n",
       "+----------+-------------+------------------+\n",
       "|2017-02-08|Councilmember| Jennifer S. Gates|\n",
       "|2017-02-08|Councilmember|Philip T. Kingston|\n",
       "+----------+-------------+------------------+"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dallas_electors = spark.read.csv('data-sources/DallasCouncilVoters.csv.gz', header=True, inferSchema=True)\n",
    "\n",
    "# cast to date\n",
    "dallas_electors = dallas_electors.withColumn(\"DATE\", F.to_date(dallas_electors[\"DATE\"], \"MM/dd/yyyy\"))\n",
    "\n",
    "dallas_electors.createOrReplaceTempView(\"dallas_electors\")\n",
    "dallas_electors.printSchema()\n",
    "dallas_electors.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28b876ad-3103-481b-94c6-a84c80cc9c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DATE: date (nullable = true)\n",
      " |-- AGENDA_ITEM_NUMBER: string (nullable = true)\n",
      " |-- ITEM_TYPE: string (nullable = true)\n",
      " |-- DISTRICT: string (nullable = true)\n",
      " |-- TITLE: string (nullable = true)\n",
      " |-- VOTER NAME: string (nullable = true)\n",
      " |-- VOTE CAST: string (nullable = true)\n",
      " |-- FINAL ACTION TAKEN: string (nullable = true)\n",
      " |-- AGENDA ITEM DESCRIPTION: string (nullable = true)\n",
      " |-- AGENDA_ID: string (nullable = true)\n",
      " |-- VOTE_ID: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>DATE</th><th>AGENDA_ITEM_NUMBER</th><th>ITEM_TYPE</th><th>DISTRICT</th><th>TITLE</th><th>VOTER NAME</th><th>VOTE CAST</th><th>FINAL ACTION TAKEN</th><th>AGENDA ITEM DESCRIPTION</th><th>AGENDA_ID</th><th>VOTE_ID</th></tr>\n",
       "<tr><td>2017-02-08</td><td>1</td><td>AGENDA</td><td>13</td><td>Councilmember</td><td>Jennifer S. Gates</td><td>N/A</td><td>NO ACTION NEEDED</td><td>Call to Order</td><td>020817__Special__1</td><td>020817__Special__...</td></tr>\n",
       "<tr><td>2017-02-08</td><td>1</td><td>AGENDA</td><td>14</td><td>Councilmember</td><td>Philip T. Kingston</td><td>N/A</td><td>NO ACTION NEEDED</td><td>Call to Order</td><td>020817__Special__1</td><td>020817__Special__...</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+------------------+---------+--------+-------------+------------------+---------+------------------+-----------------------+------------------+--------------------+\n",
       "|      DATE|AGENDA_ITEM_NUMBER|ITEM_TYPE|DISTRICT|        TITLE|        VOTER NAME|VOTE CAST|FINAL ACTION TAKEN|AGENDA ITEM DESCRIPTION|         AGENDA_ID|             VOTE_ID|\n",
       "+----------+------------------+---------+--------+-------------+------------------+---------+------------------+-----------------------+------------------+--------------------+\n",
       "|2017-02-08|                 1|   AGENDA|      13|Councilmember| Jennifer S. Gates|      N/A|  NO ACTION NEEDED|          Call to Order|020817__Special__1|020817__Special__...|\n",
       "|2017-02-08|                 1|   AGENDA|      14|Councilmember|Philip T. Kingston|      N/A|  NO ACTION NEEDED|          Call to Order|020817__Special__1|020817__Special__...|\n",
       "+----------+------------------+---------+--------+-------------+------------------+---------+------------------+-----------------------+------------------+--------------------+"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dallas_votes = spark.read.csv('data-sources/DallasCouncilVotes.csv.gz', header=True, inferSchema=True)\n",
    "\n",
    "# cast to date\n",
    "dallas_votes = dallas_votes.withColumn(\"DATE\", F.to_date(dallas_votes[\"DATE\"], \"MM/dd/yyyy\"))\n",
    "\n",
    "dallas_votes.createOrReplaceTempView(\"dallas_votes\")\n",
    "dallas_votes.printSchema()\n",
    "dallas_votes.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e668d9e4-2fe8-48aa-8944-81ffd53e1659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>name</th><th>age</th><th>city</th></tr>\n",
       "<tr><td>Amy Meyer</td><td>3</td><td>Kimberlyborough</td></tr>\n",
       "<tr><td>Amy Jones</td><td>10</td><td>Davidburgh</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+---+---------------+\n",
       "|     name|age|           city|\n",
       "+---------+---+---------------+\n",
       "|Amy Meyer|  3|Kimberlyborough|\n",
       "|Amy Jones| 10|     Davidburgh|\n",
       "+---------+---+---------------+"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people = spark.read.csv('data-sources/people_data_sample.csv', header=True, inferSchema=True)\n",
    "people.createOrReplaceTempView(\"people\")\n",
    "people.printSchema()\n",
    "people.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1e22221-fc71-4026-a372-ca6543dea502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FL_DATE: date (nullable = true)\n",
      " |-- OP_CARRIER: string (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: integer (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- ORIGIN_CITY_NAME: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- DEST_CITY_NAME: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: integer (nullable = true)\n",
      " |-- DEP_TIME: integer (nullable = true)\n",
      " |-- WHEELS_ON: integer (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- CRS_ARR_TIME: integer (nullable = true)\n",
      " |-- ARR_TIME: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>FL_DATE</th><th>OP_CARRIER</th><th>OP_CARRIER_FL_NUM</th><th>ORIGIN</th><th>ORIGIN_CITY_NAME</th><th>DEST</th><th>DEST_CITY_NAME</th><th>CRS_DEP_TIME</th><th>DEP_TIME</th><th>WHEELS_ON</th><th>TAXI_IN</th><th>CRS_ARR_TIME</th><th>ARR_TIME</th><th>CANCELLED</th><th>DISTANCE</th></tr>\n",
       "<tr><td>2000-01-01</td><td>DL</td><td>1451</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1115</td><td>1113</td><td>1343</td><td>5</td><td>1400</td><td>1348</td><td>0</td><td>946</td></tr>\n",
       "<tr><td>2000-01-01</td><td>DL</td><td>1479</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1315</td><td>1311</td><td>1536</td><td>7</td><td>1559</td><td>1543</td><td>0</td><td>946</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
       "|   FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|ORIGIN_CITY_NAME|DEST|DEST_CITY_NAME|CRS_DEP_TIME|DEP_TIME|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|CANCELLED|DISTANCE|\n",
       "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
       "|2000-01-01|        DL|             1451|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1115|    1113|     1343|      5|        1400|    1348|        0|     946|\n",
       "|2000-01-01|        DL|             1479|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1315|    1311|     1536|      7|        1559|    1543|        0|     946|\n",
       "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight = spark.read.parquet('data-sources/flight-time.parquet')\n",
    "flight.createOrReplaceTempView(\"flight\")\n",
    "flight.printSchema()\n",
    "flight.limit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9d4812-a693-4bca-ad36-d6f4f26cdc84",
   "metadata": {},
   "source": [
    "### Tables catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79396001-462c-4a29-af32-99de195527fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='dallas_electors', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='dallas_votes', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='flight', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='flights_2014', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='flights_2015', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='flights_2017', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='flights_2018', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='people', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce23b6a-4c06-4bd2-8c6a-8983be8d0f81",
   "metadata": {},
   "source": [
    "## Caching\n",
    "\n",
    "### Implementing caching\n",
    "\n",
    "Call `.cache()` on the DataFrame before Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9736ce76-b4cb-4c8a-acd2-6428f5749bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44625"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voter_df = dallas_electors.select('*')\n",
    "voter_df.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "161430cc-5e4d-4cda-8a57-f4647b10d4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+---+\n",
      "|      DATE|        TITLE|         VOTER_NAME| ID|\n",
      "+----------+-------------+-------------------+---+\n",
      "|2017-02-08|Councilmember|  Jennifer S. Gates|  1|\n",
      "|2017-02-08|Councilmember| Philip T. Kingston|  2|\n",
      "|2017-02-08|        Mayor|Michael S. Rawlings|  3|\n",
      "|2017-02-08|Councilmember|       Adam Medrano|  4|\n",
      "|2017-02-08|Councilmember|       Casey Thomas|  5|\n",
      "+----------+-------------+-------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df = voter_df.withColumn('ID', F.monotonically_increasing_id() + 1)\n",
    "voter_df = voter_df.cache()\n",
    "voter_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b54408-7bfa-49ab-ad05-d6b56d25ae5d",
   "metadata": {},
   "source": [
    "### More cache operations\n",
    "\n",
    "- Check `.is_cached` to determine cache status\n",
    "- Call `.unpersist()` when finished with DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c325474a-7774-4937-bc2b-fb8dfe96f4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(voter_df.is_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9433894-52d3-4948-a4cd-1449a49553e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>DATE</th><th>TITLE</th><th>VOTER_NAME</th><th>ID</th></tr>\n",
       "<tr><td>2017-02-08</td><td>Councilmember</td><td>Jennifer S. Gates</td><td>1</td></tr>\n",
       "<tr><td>2017-02-08</td><td>Councilmember</td><td>Philip T. Kingston</td><td>2</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+-------------+------------------+---+\n",
       "|      DATE|        TITLE|        VOTER_NAME| ID|\n",
       "+----------+-------------+------------------+---+\n",
       "|2017-02-08|Councilmember| Jennifer S. Gates|  1|\n",
       "|2017-02-08|Councilmember|Philip T. Kingston|  2|\n",
       "+----------+-------------+------------------+---+"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voter_df = voter_df.unpersist()\n",
    "voter_df.limit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faf22ad-5111-4292-ad11-b839a4a964cd",
   "metadata": {},
   "source": [
    "## Ex. 1 - Caching a DataFrame\n",
    "\n",
    "You've been assigned a task that requires running several analysis operations on a DataFrame. You've learned that caching can improve performance when reusing DataFrames and would like to implement it.\n",
    "\n",
    "You'll be working with a new dataset consisting of airline departure information. It may have repetitive data and will need to be de-duplicated.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Cache the unique rows in the `departures_df` DataFrame.\n",
    "2. Perform a count query on `departures_df`, noting how long the operation takes.\n",
    "3. Count the rows again, noting the variance in time of a cached DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7450be73-eb5a-4d52-aa0d-57be316a39f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|       2017-01-01|            5|                HNL|                          537|\n",
      "|       2017-01-01|            7|                OGG|                          498|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departures_df = flights_2017.select('*')\n",
    "departures_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5492a1b5-3924-422a-9441-80f11925af42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting 139358 rows took 1.496362 seconds\n",
      "Counting 139358 rows again took 0.448486 seconds\n"
     ]
    }
   ],
   "source": [
    "# Add caching to the unique rows in departures_df\n",
    "departures_df = departures_df.distinct().cache()\n",
    "\n",
    "# Count the unique rows in departures_df, noting how long the operation takes\n",
    "start_time = time.time()\n",
    "print(\"Counting %d rows took %f seconds\" % (departures_df.count(), time.time() - start_time))\n",
    "\n",
    "# Count the rows again, noting the variance in time of a cached DataFrame\n",
    "start_time = time.time()\n",
    "print(\"Counting %d rows again took %f seconds\" % (departures_df.count(), time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4eb44fa0-e5d6-45c7-ac30-24bfa04f9741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139358"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_2017.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2c58e6-3fb0-45e3-a82a-da26ae920e60",
   "metadata": {},
   "source": [
    "## Ex.2 - Removing a DataFrame from cache\n",
    "\n",
    "You've finished the analysis tasks with the `departures_df` DataFrame, but have some other processing to do. You'd like to remove the DataFrame from the cache to prevent any excess memory usage on your cluster.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Check the caching status on the `departures_df` DataFrame.\n",
    "2. Remove the `departures_df` DataFrame from the cache.\n",
    "3. Validate the caching status again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74ae2476-ebb9-4715-a70c-0132aa38777e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is departures_df cached?: True\n"
     ]
    }
   ],
   "source": [
    "# Determine if departures_df is in the cache\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4202ef9c-3aa1-49e5-9f90-28249bd811d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing departures_df from cache\n",
      "Is departures_df cached?: False\n"
     ]
    }
   ],
   "source": [
    "# Remove departures_df from the cache\n",
    "print(\"Removing departures_df from cache\")\n",
    "departures_df.unpersist()\n",
    "\n",
    "# Check the cache status again\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4c6189-d020-43e6-9cd5-4ae746a100b7",
   "metadata": {},
   "source": [
    "## Ex. 3 - File import performance\n",
    "\n",
    "You've been given a large set of data to import into a Spark DataFrame. You'd like to test the difference in import speed by splitting up the file.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Import the `AA_DFW_2018_Departures_Short.csv.gz` file and the `AA_DFW_2018_Departures_001.csv.gz` files into separate DataFrames.\n",
    "2. Run a count on each DataFrame and compare the run times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48faeccc-bda6-4732-bdf8-687fb171bd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the full and split files into DataFrames\n",
    "full_df = spark.read.csv('data-sources/AA_DFW_2018_Departures_Short.csv.gz')\n",
    "split_df = spark.read.csv('data-sources/AA_DFW_2018_Departures_001.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31064aab-36bb-44dc-a3f5-4beb01a2171a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in full DataFrame:\t119911\n",
      "Time to run: 0.073950\n"
     ]
    }
   ],
   "source": [
    "# Print the count and run time for each DataFrame\n",
    "start_time_a = time.time()\n",
    "print(\"Total rows in full DataFrame:\\t%d\" % full_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d390cb95-2317-4c57-a5c5-a4e06870b62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in split DataFrame:\t23982\n",
      "Time to run: 0.045212\n"
     ]
    }
   ],
   "source": [
    "start_time_b = time.time()\n",
    "print(\"Total rows in split DataFrame:\\t%d\" % split_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97797fe9-8078-4bd3-9150-f3a3f01970d7",
   "metadata": {},
   "source": [
    "## Ex. 4 - Reading Spark configurations\n",
    "\n",
    "You've recently configured a cluster via a cloud provider. Your only access is via the command shell or your python code. You'd like to verify some Spark settings to validate the configuration of the cluster.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Check the name of the Spark application instance (`'spark.app.name'`).\n",
    "2. Determine the TCP port the driver runs on (`'spark.driver.port'`).\n",
    "3. Determine how many partitions are configured for joins.\n",
    "4. Show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f0db9bb-95fc-4887-b23a-f5fec1c52a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark-shell\n",
      "Driver TCP port: 56183\n",
      "Number of partitions: 200\n"
     ]
    }
   ],
   "source": [
    "# Name of the Spark application instance\n",
    "app_name = spark.conf.get('spark.app.name')\n",
    "\n",
    "# Driver TCP port\n",
    "driver_tcp_port = spark.conf.get('spark.driver.port')\n",
    "\n",
    "# Number of join partitions\n",
    "num_partitions = spark.conf.get('spark.sql.shuffle.partitions')\n",
    "\n",
    "# Show the results\n",
    "print(\"Name: %s\" % app_name)\n",
    "print(\"Driver TCP port: %s\" % driver_tcp_port)\n",
    "print(\"Number of partitions: %s\" % num_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c483f7-7b17-48cf-9ca7-57a943943a69",
   "metadata": {},
   "source": [
    "## Ex. 5 - Writing Spark configurations\n",
    "\n",
    "Now that you've reviewed some of the Spark configurations on your cluster, you want to modify some of the settings to tune Spark to your needs. You'll import some data to review that your changes have affected the cluster.\n",
    "\n",
    "The spark configuration is initially set to the default value of 200 partitions.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Store the number of partitions in `departures_df` in the variable before.\n",
    "2. Change the `spark.sql.shuffle.partitions` configuration to 500 partitions.\n",
    "3. Recreate the `departures_df` DataFrame reading the distinct rows from the departures file.\n",
    "4. Print the number of partitions from before and after the configuration change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d164f91-52f7-4d0a-a8fc-8d9846901775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current number of partitions: 200\n",
      "Partition count before change: 2\n"
     ]
    }
   ],
   "source": [
    "# Review current partition\n",
    "print(f\"Current number of partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "\n",
    "# Load the file\n",
    "file_path = 'data-sources/AA_DFW_2018_Departures_Short.csv.gz'\n",
    "departures_df = spark.read.csv(file_path).distinct()\n",
    "\n",
    "# Review the number of partitions in the instance\n",
    "before = departures_df.rdd.getNumPartitions()\n",
    "print(\"Partition count before change: %d\" % before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30b4815b-4544-4fbd-9420-b95ea046ada8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 500\n",
      "Partition count after change: 2\n"
     ]
    }
   ],
   "source": [
    "# Configure Spark to use 500 partitions\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 500)\n",
    "print(f\"Number of partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "\n",
    "# Recreate the DataFrame\n",
    "departures_df = spark.read.csv(file_path).distinct()\n",
    "\n",
    "# Review the number of partitions in the instance\n",
    "after = departures_df.rdd.getNumPartitions()\n",
    "print(\"Partition count after change: %d\" % after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387c3fcf-2e5f-4b47-9846-df9afc55902a",
   "metadata": {},
   "source": [
    "## Ex. 6 - Normal joins\n",
    "\n",
    "You've been given two DataFrames to combine into a single useful DataFrame. Your first task is to combine the DataFrames normally and view the execution plan.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Create a new `DataFrame normal_df` by joining `flights_df` with `airports_df`.\n",
    "2. Determine which type of join is used in the query plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "935ab0e1-bf9d-4c81-983b-ea10273485b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Date (MM/DD/YYYY)</th><th>Flight Number</th><th>Destination Airport</th><th>Actual elapsed time (Minutes)</th></tr>\n",
       "<tr><td>2018-01-01</td><td>5</td><td>HNL</td><td>498</td></tr>\n",
       "<tr><td>2018-01-01</td><td>7</td><td>OGG</td><td>501</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+-------------+-------------------+-----------------------------+\n",
       "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
       "+-----------------+-------------+-------------------+-----------------------------+\n",
       "|       2018-01-01|            5|                HNL|                          498|\n",
       "|       2018-01-01|            7|                OGG|                          501|\n",
       "+-----------------+-------------+-------------------+-----------------------------+"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the data - flights\n",
    "flights_df = flights_2018.select('*')\n",
    "flights_df.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e4d88f9-c3b2-4418-b310-9c44afbb1d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>faa</th><th>name</th><th>lat</th><th>lon</th><th>alt</th><th>tz</th><th>dst</th></tr>\n",
       "<tr><td>04G</td><td>Lansdowne Airport</td><td>41.1304722</td><td>-80.6195833</td><td>1044</td><td>-5</td><td>A</td></tr>\n",
       "<tr><td>06A</td><td>Moton Field Munic...</td><td>32.4605722</td><td>-85.6800278</td><td>264</td><td>-5</td><td>A</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+--------------------+----------+-----------+----+---+---+\n",
       "|faa|                name|       lat|        lon| alt| tz|dst|\n",
       "+---+--------------------+----------+-----------+----+---+---+\n",
       "|04G|   Lansdowne Airport|41.1304722|-80.6195833|1044| -5|  A|\n",
       "|06A|Moton Field Munic...|32.4605722|-85.6800278| 264| -5|  A|\n",
       "+---+--------------------+----------+-----------+----+---+---+"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the data - airports\n",
    "airports_df = spark.read.csv('data-sources/airports.csv', header=True)\n",
    "airports_df.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "69f72264-f291-4ac9-8a83-0a3720b76dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [Destination Airport#238], [faa#1672], Inner, BuildRight, false\n",
      "   :- Project [cast(gettimestamp(Date (MM/DD/YYYY)#236, MM/dd/yyyy, TimestampType, Some(America/Regina), false) as date) AS Date (MM/DD/YYYY)#244, Flight Number#237, Destination Airport#238, Actual elapsed time (Minutes)#239]\n",
      "   :  +- Filter isnotnull(Destination Airport#238)\n",
      "   :     +- FileScan csv [Date (MM/DD/YYYY)#236,Flight Number#237,Destination Airport#238,Actual elapsed time (Minutes)#239] Batched: false, DataFilters: [isnotnull(Destination Airport#238)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/Jacqueline/Documents/projects/CAMP-PySpark/3-PySparkCle..., PartitionFilters: [], PushedFilters: [IsNotNull(Destination Airport)], ReadSchema: struct<Date (MM/DD/YYYY):string,Flight Number:int,Destination Airport:string,Actual elapsed time ...\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=1043]\n",
      "      +- Filter isnotnull(faa#1672)\n",
      "         +- FileScan csv [faa#1672,name#1673,lat#1674,lon#1675,alt#1676,tz#1677,dst#1678] Batched: false, DataFilters: [isnotnull(faa#1672)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/Jacqueline/Documents/projects/CAMP-PySpark/3-PySparkCle..., PartitionFilters: [], PushedFilters: [IsNotNull(faa)], ReadSchema: struct<faa:string,name:string,lat:string,lon:string,alt:string,tz:string,dst:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join the flights_df and aiports_df DataFrames\n",
    "normal_df = flights_df.join(airports_df,\n",
    "                            flights_df[\"Destination Airport\"] == airports_df[\"faa\"])\n",
    "\n",
    "# Show the query plan\n",
    "normal_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31a157e6-660d-4f84-aa90-d373eb5d3f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Date (MM/DD/YYYY)</th><th>Flight Number</th><th>Destination Airport</th><th>Actual elapsed time (Minutes)</th><th>faa</th><th>name</th><th>lat</th><th>lon</th><th>alt</th><th>tz</th><th>dst</th></tr>\n",
       "<tr><td>2018-01-01</td><td>5</td><td>HNL</td><td>498</td><td>HNL</td><td>Honolulu Intl</td><td>21.318681</td><td>-157.922428</td><td>13</td><td>-10</td><td>N</td></tr>\n",
       "<tr><td>2018-01-01</td><td>7</td><td>OGG</td><td>501</td><td>OGG</td><td>Kahului</td><td>20.89865</td><td>-156.430458</td><td>54</td><td>-10</td><td>N</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+-------------+-------------------+-----------------------------+---+-------------+---------+-----------+---+---+---+\n",
       "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|faa|         name|      lat|        lon|alt| tz|dst|\n",
       "+-----------------+-------------+-------------------+-----------------------------+---+-------------+---------+-----------+---+---+---+\n",
       "|       2018-01-01|            5|                HNL|                          498|HNL|Honolulu Intl|21.318681|-157.922428| 13|-10|  N|\n",
       "|       2018-01-01|            7|                OGG|                          501|OGG|      Kahului| 20.89865|-156.430458| 54|-10|  N|\n",
       "+-----------------+-------------+-------------------+-----------------------------+---+-------------+---------+-----------+---+---+---+"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_df.limit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1553a277-a534-4cce-bbab-d757d3f574b7",
   "metadata": {},
   "source": [
    "## Ex. 7 - Using broadcasting on Spark joins\n",
    "\n",
    "Remember that table joins in Spark are split between the cluster workers. If the data is not local, various shuffle operations are required and can have a negative impact on performance. Instead, we're going to use Spark's broadcast operations to give each node a copy of the specified data.\n",
    "\n",
    "A couple tips:\n",
    "- Broadcast the smaller DataFrame. The larger the DataFrame, the more time required to transfer to the worker nodes.\n",
    "- On small DataFrames, it may be better skip broadcasting and let Spark figure out any optimization on its own.\n",
    "- If you look at the query execution plan, a `broadcastHashJoin` indicates you've successfully configured broadcasting.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Import the `broadcast()` method from `pyspark.sql.functions`.\n",
    "2. Create a new DataFrame `broadcast_df` by joining `flights_df` with `airports_df`, using the broadcasting.\n",
    "3. Show the query plan and consider differences from the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12bdd04a-040c-4e74-8ed8-11289b385960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [Destination Airport#238], [faa#1672], Inner, BuildRight, false\n",
      "   :- Project [cast(gettimestamp(Date (MM/DD/YYYY)#236, MM/dd/yyyy, TimestampType, Some(America/Regina), false) as date) AS Date (MM/DD/YYYY)#244, Flight Number#237, Destination Airport#238, Actual elapsed time (Minutes)#239]\n",
      "   :  +- Filter isnotnull(Destination Airport#238)\n",
      "   :     +- FileScan csv [Date (MM/DD/YYYY)#236,Flight Number#237,Destination Airport#238,Actual elapsed time (Minutes)#239] Batched: false, DataFilters: [isnotnull(Destination Airport#238)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/Jacqueline/Documents/projects/CAMP-PySpark/3-PySparkCle..., PartitionFilters: [], PushedFilters: [IsNotNull(Destination Airport)], ReadSchema: struct<Date (MM/DD/YYYY):string,Flight Number:int,Destination Airport:string,Actual elapsed time ...\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=1212]\n",
      "      +- Filter isnotnull(faa#1672)\n",
      "         +- FileScan csv [faa#1672,name#1673,lat#1674,lon#1675,alt#1676,tz#1677,dst#1678] Batched: false, DataFilters: [isnotnull(faa#1672)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/Jacqueline/Documents/projects/CAMP-PySpark/3-PySparkCle..., PartitionFilters: [], PushedFilters: [IsNotNull(faa)], ReadSchema: struct<faa:string,name:string,lat:string,lon:string,alt:string,tz:string,dst:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join the flights_df and airports_df DataFrames using broadcasting\n",
    "broadcast_df = flights_df.join(F.broadcast(airports_df),\n",
    "                               flights_df[\"Destination Airport\"] == airports_df[\"faa\"])\n",
    "\n",
    "# Show the query plan and compare against the original\n",
    "broadcast_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78cfee3-ac15-4090-873c-20ce16a8a655",
   "metadata": {},
   "source": [
    "## Ex. 8 - Comparing broadcast vs normal joins\n",
    "\n",
    "You've created two types of joins, normal and broadcasted. Now your manager would like to know what the performance improvement is by using Spark optimizations. If the results are promising, you'll be given more opportunity to tweak the Spark setup as needed.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Execute `.count()` on the normal DataFrame.\n",
    "2. Execute `.count()` on the broadcasted DataFrame.\n",
    "3. Print the count and duration of the DataFrames noting and differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "74b7c3f8-8c54-4beb-8b88-ce3f2060d3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal count:\t\t119374\tduration: 0.235590\n",
      "Broadcast count:\t119374\tduration: 0.184753\n"
     ]
    }
   ],
   "source": [
    "# Count the number of rows in the normal DataFrame\n",
    "start_time = time.time()\n",
    "normal_count = normal_df.count()\n",
    "normal_duration = time.time() - start_time\n",
    "\n",
    "# Count the number of rows in the broadcast DataFrame\n",
    "start_time = time.time()\n",
    "broadcast_count = broadcast_df.count()\n",
    "broadcast_duration = time.time() - start_time\n",
    "\n",
    "# Print the counts and the duration of the tests\n",
    "print(\"Normal count:\\t\\t%d\\tduration: %f\" % (normal_count, normal_duration))\n",
    "print(\"Broadcast count:\\t%d\\tduration: %f\" % (broadcast_count, broadcast_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84744cb-27ee-40bb-a5c3-a8b24d7342c0",
   "metadata": {},
   "source": [
    "## Close session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a35b740-9e99-4e09-8ccc-81e0ea59e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382aa0d6-d5f7-4180-a803-f58b165acd30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
