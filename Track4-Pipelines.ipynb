{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8041e982-9ea8-43af-a191-21ceed546233",
   "metadata": {},
   "source": [
    "# Complex processing and data pipelines\n",
    "\n",
    "Learn how to process complex real-world data using Spark and the basics of pipelines.\n",
    "\n",
    "## Preparing the environment\n",
    "\n",
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84770b51-2d57-4b9d-b8ff-bc5b8773783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from pyspark.sql.types import (_parse_datatype_string, StructType, StructField, ArrayType,\n",
    "                               DoubleType, IntegerType, StringType, FloatType)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44608dc4-fe61-4d75-a337-7c8ef9d7f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore warnings coming from Arrow optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3751f7-cc7f-4c27-9055-d76887924963",
   "metadata": {},
   "source": [
    "### Connect to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d00fd800-7416-4120-b336-c915d9797bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "                     .config(\"spark.sql.repl.eagerEval.enabled\", True)  # eval DataFrame in notebooks\n",
    "                     .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffd0fb21-d1fe-49c0-abf0-15b3702f5313",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea505934-a70b-4017-9af1-e33c8399adb0",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22b83775-1081-484a-a393-9944be747d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date (MM/DD/YYYY): date (nullable = true)\n",
      " |-- Flight Number: integer (nullable = true)\n",
      " |-- Destination Airport: string (nullable = true)\n",
      " |-- Actual elapsed time (Minutes): integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Date (MM/DD/YYYY)</th><th>Flight Number</th><th>Destination Airport</th><th>Actual elapsed time (Minutes)</th></tr>\n",
       "<tr><td>2014-01-01</td><td>5</td><td>HNL</td><td>519</td></tr>\n",
       "<tr><td>2014-01-01</td><td>7</td><td>OGG</td><td>505</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+-------------+-------------------+-----------------------------+\n",
       "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
       "+-----------------+-------------+-------------------+-----------------------------+\n",
       "|       2014-01-01|            5|                HNL|                          519|\n",
       "|       2014-01-01|            7|                OGG|                          505|\n",
       "+-----------------+-------------+-------------------+-----------------------------+"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_2014 = spark.read.csv('data-sources/AA_DFW_2014_Departures_Short.csv.gz', header=True, inferSchema=True)\n",
    "\n",
    "# cast to date\n",
    "flights_2014 = flights_2014.withColumn(\"Date (MM/DD/YYYY)\", \n",
    "                                       F.to_date(flights_2014[\"Date (MM/DD/YYYY)\"], \"MM/dd/yyyy\"))\n",
    "\n",
    "flights_2014.createOrReplaceTempView(\"flights_2014\")\n",
    "flights_2014.printSchema()\n",
    "flights_2014.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b689046-34a9-4503-8e59-97f61b064c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date (MM/DD/YYYY): date (nullable = true)\n",
      " |-- Flight Number: integer (nullable = true)\n",
      " |-- Destination Airport: string (nullable = true)\n",
      " |-- Actual elapsed time (Minutes): integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Date (MM/DD/YYYY)</th><th>Flight Number</th><th>Destination Airport</th><th>Actual elapsed time (Minutes)</th></tr>\n",
       "<tr><td>2015-01-01</td><td>5</td><td>HNL</td><td>526</td></tr>\n",
       "<tr><td>2015-01-01</td><td>7</td><td>OGG</td><td>517</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+-------------+-------------------+-----------------------------+\n",
       "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
       "+-----------------+-------------+-------------------+-----------------------------+\n",
       "|       2015-01-01|            5|                HNL|                          526|\n",
       "|       2015-01-01|            7|                OGG|                          517|\n",
       "+-----------------+-------------+-------------------+-----------------------------+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_2015 = spark.read.csv('data-sources/AA_DFW_2015_Departures_Short.csv.gz', header=True, inferSchema=True)\n",
    "\n",
    "# cast to date\n",
    "flights_2015 = flights_2015.withColumn(\"Date (MM/DD/YYYY)\", \n",
    "                                       F.to_date(flights_2015[\"Date (MM/DD/YYYY)\"], \"MM/dd/yyyy\"))\n",
    "\n",
    "flights_2015.createOrReplaceTempView(\"flights_2015\")\n",
    "flights_2015.printSchema()\n",
    "flights_2015.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96fcc16c-b1ea-4219-9833-ad050a1d25d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date (MM/DD/YYYY): date (nullable = true)\n",
      " |-- Flight Number: integer (nullable = true)\n",
      " |-- Destination Airport: string (nullable = true)\n",
      " |-- Actual elapsed time (Minutes): integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Date (MM/DD/YYYY)</th><th>Flight Number</th><th>Destination Airport</th><th>Actual elapsed time (Minutes)</th></tr>\n",
       "<tr><td>2017-01-01</td><td>5</td><td>HNL</td><td>537</td></tr>\n",
       "<tr><td>2017-01-01</td><td>7</td><td>OGG</td><td>498</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+-------------+-------------------+-----------------------------+\n",
       "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
       "+-----------------+-------------+-------------------+-----------------------------+\n",
       "|       2017-01-01|            5|                HNL|                          537|\n",
       "|       2017-01-01|            7|                OGG|                          498|\n",
       "+-----------------+-------------+-------------------+-----------------------------+"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_2017 = spark.read.csv('data-sources/AA_DFW_2017_Departures_Short.csv.gz', header=True, inferSchema=True)\n",
    "\n",
    "# cast to date\n",
    "flights_2017 = flights_2017.withColumn(\"Date (MM/DD/YYYY)\", \n",
    "                                       F.to_date(flights_2017[\"Date (MM/DD/YYYY)\"], \"MM/dd/yyyy\"))\n",
    "flights_2017.createOrReplaceTempView(\"flights_2017\")\n",
    "flights_2017.printSchema()\n",
    "flights_2017.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfbb6415-4ff7-417a-99ea-2851ce4ac093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date (MM/DD/YYYY): date (nullable = true)\n",
      " |-- Flight Number: integer (nullable = true)\n",
      " |-- Destination Airport: string (nullable = true)\n",
      " |-- Actual elapsed time (Minutes): integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Date (MM/DD/YYYY)</th><th>Flight Number</th><th>Destination Airport</th><th>Actual elapsed time (Minutes)</th></tr>\n",
       "<tr><td>2018-01-01</td><td>5</td><td>HNL</td><td>498</td></tr>\n",
       "<tr><td>2018-01-01</td><td>7</td><td>OGG</td><td>501</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+-------------+-------------------+-----------------------------+\n",
       "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
       "+-----------------+-------------+-------------------+-----------------------------+\n",
       "|       2018-01-01|            5|                HNL|                          498|\n",
       "|       2018-01-01|            7|                OGG|                          501|\n",
       "+-----------------+-------------+-------------------+-----------------------------+"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_2018 = spark.read.csv('data-sources/AA_DFW_2018_Departures_Short.csv.gz', header=True, inferSchema=True)\n",
    "\n",
    "# cast to date\n",
    "flights_2018 = flights_2018.withColumn(\"Date (MM/DD/YYYY)\", \n",
    "                                       F.to_date(flights_2018[\"Date (MM/DD/YYYY)\"], \"MM/dd/yyyy\"))\n",
    "\n",
    "# save the file in csv fprmat\n",
    "(flights_2018.repartition(5)\n",
    "             .write.format('csv')\n",
    "             .save('output-files/AA_DFW_2018_Departures_Short.csv', mode='overwrite'))\n",
    "\n",
    "# Review the data\n",
    "flights_2018.createOrReplaceTempView(\"flights_2018\")\n",
    "flights_2018.printSchema()\n",
    "flights_2018.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da08c587-0bbe-4a65-955d-f35c931ff7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DATE: date (nullable = true)\n",
      " |-- TITLE: string (nullable = true)\n",
      " |-- VOTER_NAME: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>DATE</th><th>TITLE</th><th>VOTER_NAME</th></tr>\n",
       "<tr><td>2017-02-08</td><td>Councilmember</td><td>Jennifer S. Gates</td></tr>\n",
       "<tr><td>2017-02-08</td><td>Councilmember</td><td>Philip T. Kingston</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+-------------+------------------+\n",
       "|      DATE|        TITLE|        VOTER_NAME|\n",
       "+----------+-------------+------------------+\n",
       "|2017-02-08|Councilmember| Jennifer S. Gates|\n",
       "|2017-02-08|Councilmember|Philip T. Kingston|\n",
       "+----------+-------------+------------------+"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dallas_electors = spark.read.csv('data-sources/DallasCouncilVoters.csv.gz', header=True, inferSchema=True)\n",
    "\n",
    "# cast to date\n",
    "dallas_electors = dallas_electors.withColumn(\"DATE\", F.to_date(dallas_electors[\"DATE\"], \"MM/dd/yyyy\"))\n",
    "\n",
    "dallas_electors.createOrReplaceTempView(\"dallas_electors\")\n",
    "dallas_electors.printSchema()\n",
    "dallas_electors.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28b876ad-3103-481b-94c6-a84c80cc9c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DATE: date (nullable = true)\n",
      " |-- AGENDA_ITEM_NUMBER: string (nullable = true)\n",
      " |-- ITEM_TYPE: string (nullable = true)\n",
      " |-- DISTRICT: string (nullable = true)\n",
      " |-- TITLE: string (nullable = true)\n",
      " |-- VOTER NAME: string (nullable = true)\n",
      " |-- VOTE CAST: string (nullable = true)\n",
      " |-- FINAL ACTION TAKEN: string (nullable = true)\n",
      " |-- AGENDA ITEM DESCRIPTION: string (nullable = true)\n",
      " |-- AGENDA_ID: string (nullable = true)\n",
      " |-- VOTE_ID: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>DATE</th><th>AGENDA_ITEM_NUMBER</th><th>ITEM_TYPE</th><th>DISTRICT</th><th>TITLE</th><th>VOTER NAME</th><th>VOTE CAST</th><th>FINAL ACTION TAKEN</th><th>AGENDA ITEM DESCRIPTION</th><th>AGENDA_ID</th><th>VOTE_ID</th></tr>\n",
       "<tr><td>2017-02-08</td><td>1</td><td>AGENDA</td><td>13</td><td>Councilmember</td><td>Jennifer S. Gates</td><td>N/A</td><td>NO ACTION NEEDED</td><td>Call to Order</td><td>020817__Special__1</td><td>020817__Special__...</td></tr>\n",
       "<tr><td>2017-02-08</td><td>1</td><td>AGENDA</td><td>14</td><td>Councilmember</td><td>Philip T. Kingston</td><td>N/A</td><td>NO ACTION NEEDED</td><td>Call to Order</td><td>020817__Special__1</td><td>020817__Special__...</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+------------------+---------+--------+-------------+------------------+---------+------------------+-----------------------+------------------+--------------------+\n",
       "|      DATE|AGENDA_ITEM_NUMBER|ITEM_TYPE|DISTRICT|        TITLE|        VOTER NAME|VOTE CAST|FINAL ACTION TAKEN|AGENDA ITEM DESCRIPTION|         AGENDA_ID|             VOTE_ID|\n",
       "+----------+------------------+---------+--------+-------------+------------------+---------+------------------+-----------------------+------------------+--------------------+\n",
       "|2017-02-08|                 1|   AGENDA|      13|Councilmember| Jennifer S. Gates|      N/A|  NO ACTION NEEDED|          Call to Order|020817__Special__1|020817__Special__...|\n",
       "|2017-02-08|                 1|   AGENDA|      14|Councilmember|Philip T. Kingston|      N/A|  NO ACTION NEEDED|          Call to Order|020817__Special__1|020817__Special__...|\n",
       "+----------+------------------+---------+--------+-------------+------------------+---------+------------------+-----------------------+------------------+--------------------+"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dallas_votes = spark.read.csv('data-sources/DallasCouncilVotes.csv.gz', header=True, inferSchema=True)\n",
    "\n",
    "# cast to date\n",
    "dallas_votes = dallas_votes.withColumn(\"DATE\", F.to_date(dallas_votes[\"DATE\"], \"MM/dd/yyyy\"))\n",
    "\n",
    "dallas_votes.createOrReplaceTempView(\"dallas_votes\")\n",
    "dallas_votes.printSchema()\n",
    "dallas_votes.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e668d9e4-2fe8-48aa-8944-81ffd53e1659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>name</th><th>age</th><th>city</th></tr>\n",
       "<tr><td>Amy Meyer</td><td>3</td><td>Kimberlyborough</td></tr>\n",
       "<tr><td>Amy Jones</td><td>10</td><td>Davidburgh</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+---+---------------+\n",
       "|     name|age|           city|\n",
       "+---------+---+---------------+\n",
       "|Amy Meyer|  3|Kimberlyborough|\n",
       "|Amy Jones| 10|     Davidburgh|\n",
       "+---------+---+---------------+"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people = spark.read.csv('data-sources/people_data_sample.csv', header=True, inferSchema=True)\n",
    "people.createOrReplaceTempView(\"people\")\n",
    "people.printSchema()\n",
    "people.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1e22221-fc71-4026-a372-ca6543dea502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FL_DATE: date (nullable = true)\n",
      " |-- OP_CARRIER: string (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: integer (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- ORIGIN_CITY_NAME: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- DEST_CITY_NAME: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: integer (nullable = true)\n",
      " |-- DEP_TIME: integer (nullable = true)\n",
      " |-- WHEELS_ON: integer (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- CRS_ARR_TIME: integer (nullable = true)\n",
      " |-- ARR_TIME: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>FL_DATE</th><th>OP_CARRIER</th><th>OP_CARRIER_FL_NUM</th><th>ORIGIN</th><th>ORIGIN_CITY_NAME</th><th>DEST</th><th>DEST_CITY_NAME</th><th>CRS_DEP_TIME</th><th>DEP_TIME</th><th>WHEELS_ON</th><th>TAXI_IN</th><th>CRS_ARR_TIME</th><th>ARR_TIME</th><th>CANCELLED</th><th>DISTANCE</th></tr>\n",
       "<tr><td>2000-01-01</td><td>DL</td><td>1451</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1115</td><td>1113</td><td>1343</td><td>5</td><td>1400</td><td>1348</td><td>0</td><td>946</td></tr>\n",
       "<tr><td>2000-01-01</td><td>DL</td><td>1479</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1315</td><td>1311</td><td>1536</td><td>7</td><td>1559</td><td>1543</td><td>0</td><td>946</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
       "|   FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|ORIGIN_CITY_NAME|DEST|DEST_CITY_NAME|CRS_DEP_TIME|DEP_TIME|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|CANCELLED|DISTANCE|\n",
       "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
       "|2000-01-01|        DL|             1451|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1115|    1113|     1343|      5|        1400|    1348|        0|     946|\n",
       "|2000-01-01|        DL|             1479|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1315|    1311|     1536|      7|        1559|    1543|        0|     946|\n",
       "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight = spark.read.parquet('data-sources/flight-time.parquet')\n",
    "flight.createOrReplaceTempView(\"flight\")\n",
    "flight.printSchema()\n",
    "flight.limit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9d4812-a693-4bca-ad36-d6f4f26cdc84",
   "metadata": {},
   "source": [
    "### Tables catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79396001-462c-4a29-af32-99de195527fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='dallas_electors', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='dallas_votes', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='flight', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='flights_2014', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='flights_2015', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='flights_2017', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='flights_2018', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='people', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31258292-2735-4687-a773-e5cb53d5c6a3",
   "metadata": {},
   "source": [
    "## Introduction to data pipelines\n",
    "\n",
    "### Pipeline details\n",
    "\n",
    "- Not formally defined in Spark\r",
    "- \n",
    "Typically all normal Spark code required for task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09485d99-ffe5-487b-9ab4-4c2f4537f7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('name', StringType(), False),\n",
    "    StructField('age', StringType(), False),\n",
    "    StructField('city', StringType(), False),\n",
    "])\n",
    "df = spark.read.format('csv').schema(schema).load('data-sources/people_data_sample.csv')\n",
    "df = df.withColumn('id', F.monotonically_increasing_id()+1)\n",
    "df.write.parquet('output-files/people_data_sample.parquet', mode='overwrite')\n",
    "df.write.json('output-files/outdata.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf5540b-3a93-4cd7-8a47-bf578e36311f",
   "metadata": {},
   "source": [
    "## Ex. 1 - Quick pipeline\n",
    "\n",
    "Before you parse some more complex data, your manager would like to see a simple pipeline example including the basic steps. For this example, you'll want to ingest a data file, filter a few rows, add an ID column to it, then write it out as JSON data.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Import the file `AA_DFW_2015_Departures_Short.csv.gz` to a DataFrame. Note the header is already defined.\n",
    "2. Filter the DataFrame to contain only flights with a duration over 0 minutes. Use the index of the column, not the column name (remember to use `.printSchema()` to see the column names / order).\n",
    "3. Add an `ID` column.\n",
    "4. Write the file out as a JSON document named `AA_DFW_2015_output.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0477c5f1-cb33-4aeb-81b4-04849546c781",
   "metadata": {},
   "source": [
    "### Pipeline - all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2ed6dff-d3bb-48c4-998d-3b10e5027fea",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Import the data to a DataFrame\n",
    "departures_df = spark.read.csv('data-sources/AA_DFW_2015_Departures_Short.csv.gz', header=True)\n",
    "\n",
    "# Remove any duration of 0\n",
    "departures_df = departures_df.filter(departures_df['Actual elapsed time (Minutes)'] != 0)\n",
    "\n",
    "# Add an ID column\n",
    "departures_df = departures_df.withColumn('id', F.monotonically_increasing_id())\n",
    "\n",
    "# Write the file out to JSON format\n",
    "departures_df.write.json('output-files/AA_DFW_2015_output.json', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27af187-2430-4d97-aee5-7b1ba95f1a21",
   "metadata": {},
   "source": [
    "### Explore a litte bit more the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a02360b-1853-4b59-ba2d-8b47e8ea0280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date (MM/DD/YYYY): string (nullable = true)\n",
      " |-- Flight Number: string (nullable = true)\n",
      " |-- Destination Airport: string (nullable = true)\n",
      " |-- Actual elapsed time (Minutes): string (nullable = true)\n",
      " |-- id: long (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Date (MM/DD/YYYY)</th><th>Flight Number</th><th>Destination Airport</th><th>Actual elapsed time (Minutes)</th><th>id</th></tr>\n",
       "<tr><td>01/01/2015</td><td>0005</td><td>HNL</td><td>526</td><td>0</td></tr>\n",
       "<tr><td>01/01/2015</td><td>0007</td><td>OGG</td><td>517</td><td>1</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+-------------+-------------------+-----------------------------+---+\n",
       "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)| id|\n",
       "+-----------------+-------------+-------------------+-----------------------------+---+\n",
       "|       01/01/2015|         0005|                HNL|                          526|  0|\n",
       "|       01/01/2015|         0007|                OGG|                          517|  1|\n",
       "+-----------------+-------------+-------------------+-----------------------------+---+"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the data to a DataFrame\n",
    "departures_df.printSchema()\n",
    "departures_df.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e3d03df-2121-4d4e-b85c-311f93cff903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Actual elapsed time (Minutes)</th></tr>\n",
       "<tr><td>526</td></tr>\n",
       "<tr><td>517</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------------------+\n",
       "|Actual elapsed time (Minutes)|\n",
       "+-----------------------------+\n",
       "|                          526|\n",
       "|                          517|\n",
       "+-----------------------------+"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using column index to select the data to show\n",
    "departures_df.select(departures_df.columns[3]).limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f568c8c-e513-413a-8c57-fc1f13acfd2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Date (MM/DD/YYYY)</th><th>Flight Number</th><th>Destination Airport</th><th>Actual elapsed time (Minutes)</th><th>id</th></tr>\n",
       "<tr><td>02/27/2015</td><td>0007</td><td>OGG</td><td>679</td><td>22142</td></tr>\n",
       "<tr><td>04/24/2015</td><td>0007</td><td>OGG</td><td>631</td><td>43929</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+-------------+-------------------+-----------------------------+-----+\n",
       "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|   id|\n",
       "+-----------------+-------------+-------------------+-----------------------------+-----+\n",
       "|       02/27/2015|         0007|                OGG|                          679|22142|\n",
       "|       04/24/2015|         0007|                OGG|                          631|43929|\n",
       "+-----------------+-------------+-------------------+-----------------------------+-----+"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering the data with the column index\n",
    "departures_df.filter(departures_df[3] > 600).limit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dce0286-7623-4438-b5d2-9b67436fff84",
   "metadata": {},
   "source": [
    "## Ex. 2 - Removing commented lines\n",
    "Your boss would like you to perform some complex parsing on a new dataset. The data represents annotation data for the ImageNet dataset, but focusing specifically on dog breeds and identifying them in images. Before any actual analysis can occur, you'll need to clear out several components of invalid / incorrect data. The general schema of the document is unknown so you'd like to import the rows into a single column, allowing for quick analysis.\n",
    "\n",
    "To start, you need to remove all commented rows in the dataset.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Import the `annotations.csv.gz` file to a DataFrame and perform a row count. Specify a separator character of `|`.\n",
    "2. Query the data for the number of rows beginning with `#`.\n",
    "3. Import the file again to a new DataFrame, but specify the comment character in the options to remove any commented rows.\n",
    "4. Count the new DataFrame and verify the difference is as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24ba453c-b42a-4d87-b524-bbaed1778399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File location\n",
    "path_file = 'data-sources/annotations-sample-file.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d92e26f2-4134-479f-b213-34a0e1953709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 32794\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>_c0</th><th>_c1</th></tr>\n",
       "<tr><td>025865917\\tn02352...</td><td>Temp</td></tr>\n",
       "<tr><td>022684404\\tn02938...</td><td>Temp</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+----+\n",
       "|                 _c0| _c1|\n",
       "+--------------------+----+\n",
       "|025865917\\tn02352...|Temp|\n",
       "|022684404\\tn02938...|Temp|\n",
       "+--------------------+----+"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the file to a DataFrame and perform a row count\n",
    "annotations_df = spark.read.csv(path_file, sep='|')\n",
    "full_count = annotations_df.count()\n",
    "\n",
    "print(f'Total rows: {full_count}')\n",
    "annotations_df.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1199fd9-2608-4818-a719-7515cc084dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1416"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of rows beginning with '#'\n",
    "comment_count = annotations_df.where(F.col('_c0').startswith('#')).count()\n",
    "comment_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39a323ef-22c2-475b-b04c-815c9b1d0e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 31378\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>_c0</th><th>_c1</th></tr>\n",
       "<tr><td>025865917\\tn02352...</td><td>Temp</td></tr>\n",
       "<tr><td>022684404\\tn02938...</td><td>Temp</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+----+\n",
       "|                 _c0| _c1|\n",
       "+--------------------+----+\n",
       "|025865917\\tn02352...|Temp|\n",
       "|022684404\\tn02938...|Temp|\n",
       "+--------------------+----+"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the file to a new DataFrame, without commented rows\n",
    "no_comments_df = spark.read.csv(path_file, sep='|', comment='#')\n",
    "no_comments_count = no_comments_df.count()\n",
    "\n",
    "print(f'Total rows: {no_comments_count}')\n",
    "no_comments_df.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "176f860a-846a-4316-ba27-9aa2a43eb31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full count: 32794\n",
      "Comment count: 1416\n",
      "Remaining count: 31378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the new DataFrame and verify the difference is as expected\n",
    "print(f'''\n",
    "Full count: {full_count}\n",
    "Comment count: {comment_count}\n",
    "Remaining count: {no_comments_count}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037f3a94-b5b2-4021-814a-6f9e4a8286ec",
   "metadata": {},
   "source": [
    "## Ex. 3 - Removing invalid rows\n",
    "\n",
    "Now that you've successfully removed the commented rows, you have received some information about the general format of the data. There should be at minimum 5 tab separated columns in the DataFrame. Remember that your original DataFrame only has a single column, so you'll need to split the data on the tab (`\\t`) characters.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Create a new variable `tmp_fields` using the `annotations_df` DataFrame column `'_c0'` splitting it on the tab character.\n",
    "2. Create a new column in `annotations_df` named `'colcount'` representing the number of fields defined in the previous step.\n",
    "3. Filter out any rows from `annotations_df` containing fewer than 5 fields.\n",
    "4. Count the number of rows in the DataFrame and compare to the `initial_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36b55c31-e088-4211-bd0e-e73e94116d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 31378\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>_c0</th><th>_c1</th></tr>\n",
       "<tr><td>025865917\\tn02352...</td><td>Temp</td></tr>\n",
       "<tr><td>022684404\\tn02938...</td><td>Temp</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+----+\n",
       "|                 _c0| _c1|\n",
       "+--------------------+----+\n",
       "|025865917\\tn02352...|Temp|\n",
       "|022684404\\tn02938...|Temp|\n",
       "+--------------------+----+"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the cleaned database (no comments)\n",
    "annotations_df = no_comments_df.select('*')\n",
    "initial_count = annotations_df.count()\n",
    "print(f'Total rows: {initial_count}')\n",
    "annotations_df.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5630ce5-ab86-4496-a0f4-326bc6974b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>_c0</th><th>_c1</th><th>tmp_fields</th></tr>\n",
       "<tr><td>025865917\\tn02352...</td><td>Temp</td><td>[025865917, n0235...</td></tr>\n",
       "<tr><td>022684404\\tn02938...</td><td>Temp</td><td>[022684404, n0293...</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+----+--------------------+\n",
       "|                 _c0| _c1|          tmp_fields|\n",
       "+--------------------+----+--------------------+\n",
       "|025865917\\tn02352...|Temp|[025865917, n0235...|\n",
       "|022684404\\tn02938...|Temp|[022684404, n0293...|\n",
       "+--------------------+----+--------------------+"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split _c0 on the tab character and store the list in a variable\n",
    "tmp_fields = F.split(annotations_df['_c0'], '\\t')\n",
    "annotations_df.withColumn('tmp_fields', tmp_fields).limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23d99b18-a872-4024-9572-142560327e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 31378\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>_c0</th><th>_c1</th><th>colcount</th></tr>\n",
       "<tr><td>025865917\\tn02352...</td><td>Temp</td><td>2</td></tr>\n",
       "<tr><td>022684404\\tn02938...</td><td>Temp</td><td>2</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+----+--------+\n",
       "|                 _c0| _c1|colcount|\n",
       "+--------------------+----+--------+\n",
       "|025865917\\tn02352...|Temp|       2|\n",
       "|022684404\\tn02938...|Temp|       2|\n",
       "+--------------------+----+--------+"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the colcount column on the DataFrame\n",
    "annotations_df = annotations_df.withColumn('colcount', F.size(tmp_fields))\n",
    "print(f'Total rows: {annotations_df.count()}')\n",
    "annotations_df.orderBy(annotations_df.colcount).limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a458a19-7a55-456b-8389-adf5f410491e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 20580\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>_c0</th><th>_c1</th><th>colcount</th></tr>\n",
       "<tr><td>02110627\\tn021106...</td><td>Temp</td><td>5</td></tr>\n",
       "<tr><td>02093754\\tn020937...</td><td>Temp</td><td>5</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+----+--------+\n",
       "|                 _c0| _c1|colcount|\n",
       "+--------------------+----+--------+\n",
       "|02110627\\tn021106...|Temp|       5|\n",
       "|02093754\\tn020937...|Temp|       5|\n",
       "+--------------------+----+--------+"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove any rows containing fewer than 5 fields\n",
    "annotations_df_filtered = annotations_df.filter(~ (annotations_df.colcount < 5))\n",
    "final_count = annotations_df_filtered.count()\n",
    "print(f'Total rows: {final_count}')\n",
    "annotations_df_filtered.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afb10810-61db-40b1-af48-cc59eb35df72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial count: 31378\n",
      "Final count:   20580\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the number of rows\n",
    "print(f'''\n",
    "Initial count: {initial_count}\n",
    "Final count:   {final_count}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e2ed4c-ec4e-45aa-92c1-dbe2308a9639",
   "metadata": {},
   "source": [
    "## Ex. 4 - Splitting into columns\n",
    "\n",
    "You've cleaned up your data considerably by removing the invalid rows from the DataFrame. Now you want to perform some further transformations by generating specific meaningful columns based on the DataFrame content.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Split the content of the `'_c0'` column on the tab character and store in a variable called `split_cols`.\n",
    "2. Add the following columns based on the first four entries in the variable above: `folder`, `filename`, `width`, `height` on a DataFrame named `split_df`.\n",
    "3. Add the `split_cols` variable as a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91737d59-b5e1-4cd1-9eb2-3dd6b6f19e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 20580\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>_c0</th><th>_c1</th><th>colcount</th></tr>\n",
       "<tr><td>02110627\\tn021106...</td><td>Temp</td><td>5</td></tr>\n",
       "<tr><td>02093754\\tn020937...</td><td>Temp</td><td>5</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+----+--------+\n",
       "|                 _c0| _c1|colcount|\n",
       "+--------------------+----+--------+\n",
       "|02110627\\tn021106...|Temp|       5|\n",
       "|02093754\\tn020937...|Temp|       5|\n",
       "+--------------------+----+--------+"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the cleaned dataframe\n",
    "annotations_df = annotations_df_filtered.select('*')\n",
    "print(f'Total rows: {annotations_df.count()}')\n",
    "annotations_df.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26577c8a-83c1-4120-9b93-7f5f0297e326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 20580\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>_c0</th><th>_c1</th><th>colcount</th><th>folder</th><th>filename</th><th>width</th><th>height</th><th>split_cols</th></tr>\n",
       "<tr><td>02110627\\tn021106...</td><td>Temp</td><td>5</td><td>02110627</td><td>n02110627_12938</td><td>200</td><td>300</td><td>[02110627, n02110...</td></tr>\n",
       "<tr><td>02093754\\tn020937...</td><td>Temp</td><td>5</td><td>02093754</td><td>n02093754_1148</td><td>500</td><td>378</td><td>[02093754, n02093...</td></tr>\n",
       "<tr><td>%s\\t%s\\t800\\t600\\...</td><td>Temp</td><td>5</td><td>%s</td><td>%s</td><td>800</td><td>600</td><td>[%s, %s, 800, 600...</td></tr>\n",
       "<tr><td>02104029\\tn021040...</td><td>Temp</td><td>5</td><td>02104029</td><td>n02104029_63</td><td>500</td><td>375</td><td>[02104029, n02104...</td></tr>\n",
       "<tr><td>02111500\\tn021115...</td><td>Temp</td><td>5</td><td>02111500</td><td>n02111500_5137</td><td>500</td><td>375</td><td>[02111500, n02111...</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+----+--------+--------+---------------+-----+------+--------------------+\n",
       "|                 _c0| _c1|colcount|  folder|       filename|width|height|          split_cols|\n",
       "+--------------------+----+--------+--------+---------------+-----+------+--------------------+\n",
       "|02110627\\tn021106...|Temp|       5|02110627|n02110627_12938|  200|   300|[02110627, n02110...|\n",
       "|02093754\\tn020937...|Temp|       5|02093754| n02093754_1148|  500|   378|[02093754, n02093...|\n",
       "|%s\\t%s\\t800\\t600\\...|Temp|       5|      %s|             %s|  800|   600|[%s, %s, 800, 600...|\n",
       "|02104029\\tn021040...|Temp|       5|02104029|   n02104029_63|  500|   375|[02104029, n02104...|\n",
       "|02111500\\tn021115...|Temp|       5|02111500| n02111500_5137|  500|   375|[02111500, n02111...|\n",
       "+--------------------+----+--------+--------+---------------+-----+------+--------------------+"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the content of _c0 on the tab character (aka, '\\t')\n",
    "split_cols = F.split(annotations_df['_c0'], '\\t')\n",
    "\n",
    "# Add the columns folder, filename, width, and height\n",
    "split_df = annotations_df.withColumn('folder', split_cols.getItem(0))\n",
    "split_df = split_df.withColumn('filename', split_cols.getItem(1))\n",
    "split_df = split_df.withColumn('width', split_cols.getItem(2))\n",
    "split_df = split_df.withColumn('height', split_cols.getItem(3))\n",
    "\n",
    "# Add split_cols as a column\n",
    "split_df = split_df.withColumn('split_cols', split_cols)\n",
    "print(f'Total rows: {split_df.count()}')\n",
    "split_df.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16d6b9a-c6ff-4c29-b106-d5b1e9df3c0b",
   "metadata": {},
   "source": [
    "## Ex. 5 - Further parsing\n",
    "\n",
    "You've molded this dataset into a significantly different format than it was before, but there are still a few things left to do. You need to prep the column data for use in later analysis and remove a few intermediary columns.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Create a new function called retriever that takes two arguments, the split columns (cols) and the total number of columns (colcount). This function should return a list of the entries that have not been defined as columns yet (i.e., everything after item 4 in the list).\n",
    "2. Define the function as a Spark UDF, returning an Array of strings.\n",
    "3. Create the new column `dog_list` using the UDF and the available columns in the DataFrame.\n",
    "4. Remove the columns `_c0`, `colcount`, and `split_cols`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2ecc951-aca6-4a45-835a-c4e330c71bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 20580\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>_c0</th><th>_c1</th><th>colcount</th><th>folder</th><th>filename</th><th>width</th><th>height</th><th>split_cols</th></tr>\n",
       "<tr><td>02110627\\tn021106...</td><td>Temp</td><td>5</td><td>02110627</td><td>n02110627_12938</td><td>200</td><td>300</td><td>[02110627, n02110...</td></tr>\n",
       "<tr><td>02093754\\tn020937...</td><td>Temp</td><td>5</td><td>02093754</td><td>n02093754_1148</td><td>500</td><td>378</td><td>[02093754, n02093...</td></tr>\n",
       "<tr><td>%s\\t%s\\t800\\t600\\...</td><td>Temp</td><td>5</td><td>%s</td><td>%s</td><td>800</td><td>600</td><td>[%s, %s, 800, 600...</td></tr>\n",
       "<tr><td>02104029\\tn021040...</td><td>Temp</td><td>5</td><td>02104029</td><td>n02104029_63</td><td>500</td><td>375</td><td>[02104029, n02104...</td></tr>\n",
       "<tr><td>02111500\\tn021115...</td><td>Temp</td><td>5</td><td>02111500</td><td>n02111500_5137</td><td>500</td><td>375</td><td>[02111500, n02111...</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+----+--------+--------+---------------+-----+------+--------------------+\n",
       "|                 _c0| _c1|colcount|  folder|       filename|width|height|          split_cols|\n",
       "+--------------------+----+--------+--------+---------------+-----+------+--------------------+\n",
       "|02110627\\tn021106...|Temp|       5|02110627|n02110627_12938|  200|   300|[02110627, n02110...|\n",
       "|02093754\\tn020937...|Temp|       5|02093754| n02093754_1148|  500|   378|[02093754, n02093...|\n",
       "|%s\\t%s\\t800\\t600\\...|Temp|       5|      %s|             %s|  800|   600|[%s, %s, 800, 600...|\n",
       "|02104029\\tn021040...|Temp|       5|02104029|   n02104029_63|  500|   375|[02104029, n02104...|\n",
       "|02111500\\tn021115...|Temp|       5|02111500| n02111500_5137|  500|   375|[02111500, n02111...|\n",
       "+--------------------+----+--------+--------+---------------+-----+------+--------------------+"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reviewing the data\n",
    "print(f'Total rows: {split_df.count()}')\n",
    "split_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64b3e8bc-881a-431e-bea2-7933c4e8c6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 20580\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>_c0</th><th>_c1</th><th>colcount</th><th>folder</th><th>filename</th><th>width</th><th>height</th><th>split_cols</th><th>dog_list</th></tr>\n",
       "<tr><td>02110627\\tn021106...</td><td>Temp</td><td>5</td><td>02110627</td><td>n02110627_12938</td><td>200</td><td>300</td><td>[02110627, n02110...</td><td>[affenpinscher,0,...</td></tr>\n",
       "<tr><td>02093754\\tn020937...</td><td>Temp</td><td>5</td><td>02093754</td><td>n02093754_1148</td><td>500</td><td>378</td><td>[02093754, n02093...</td><td>[Border_terrier,7...</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+----+--------+--------+---------------+-----+------+--------------------+--------------------+\n",
       "|                 _c0| _c1|colcount|  folder|       filename|width|height|          split_cols|            dog_list|\n",
       "+--------------------+----+--------+--------+---------------+-----+------+--------------------+--------------------+\n",
       "|02110627\\tn021106...|Temp|       5|02110627|n02110627_12938|  200|   300|[02110627, n02110...|[affenpinscher,0,...|\n",
       "|02093754\\tn020937...|Temp|       5|02093754| n02093754_1148|  500|   378|[02093754, n02093...|[Border_terrier,7...|\n",
       "+--------------------+----+--------+--------+---------------+-----+------+--------------------+--------------------+"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a Python method\n",
    "def retriever(cols, colcount):\n",
    "    '''Return a list of dog data (remaining items in list after position 4)'''\n",
    "    return cols[4:colcount]\n",
    "\n",
    "# Wrap the function and store as a variable\n",
    "udfRetriever = F.udf(retriever, ArrayType(StringType()))\n",
    "\n",
    "# Use with Spark\n",
    "dog_df = split_df.withColumn('dog_list', udfRetriever(split_df.split_cols, split_df.colcount))\n",
    "print(f'Total rows: {dog_df.count()}')\n",
    "dog_df.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6bf9407-5f9e-4e9c-abb3-6a36ed858137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 20580\n",
      "+--------+---------------+-----+------+----------------------------------+\n",
      "|folder  |filename       |width|height|dog_list                          |\n",
      "+--------+---------------+-----+------+----------------------------------+\n",
      "|02110627|n02110627_12938|200  |300   |[affenpinscher,0,9,173,298]       |\n",
      "|02093754|n02093754_1148 |500  |378   |[Border_terrier,73,127,341,335]   |\n",
      "|%s      |%s             |800  |600   |[Shetland_sheepdog,124,87,576,514]|\n",
      "|02104029|n02104029_63   |500  |375   |[kuvasz,0,0,499,327]              |\n",
      "|02111500|n02111500_5137 |500  |375   |[Great_Pyrenees,124,225,403,374]  |\n",
      "|02104365|n02104365_7518 |500  |333   |[schipperke,146,29,416,309]       |\n",
      "|02105056|n02105056_2834 |500  |375   |[groenendael,168,0,469,374]       |\n",
      "|02093647|n02093647_541  |500  |333   |[Bedlington_terrier,10,12,462,332]|\n",
      "|02098413|n02098413_1355 |500  |375   |[Lhasa,39,1,499,373]              |\n",
      "|02093859|n02093859_2309 |330  |500   |[Kerry_blue_terrier,17,16,300,482]|\n",
      "+--------+---------------+-----+------+----------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove unused columns\n",
    "clean_dog_df = dog_df.drop('_c0').drop('_c1').drop('split_cols').drop('colcount')\n",
    "print(f'Total rows: {clean_dog_df.count()}')\n",
    "clean_dog_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8a1783-c10b-458e-ac72-9e2fbc495aab",
   "metadata": {},
   "source": [
    "## Ex. 6 - Validate rows via join\n",
    "\n",
    "Another example of filtering data is using joins to remove invalid entries. You'll need to verify the folder names are as expected based on a given DataFrame named `valid_folders_df`. The DataFrame `clean_dog_df` is as you last left it with a group of split columns.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Rename the `_c0` column to folder on the `valid_folders_df` DataFrame.\n",
    "2. Count the number of rows in `clean_dog_df`.\n",
    "3. Join the two DataFrames on the folder name, and call the resulting DataFrame `joined_df`. Make sure to broadcast the smaller DataFrame.\n",
    "4. Check the number of rows remaining in the DataFrame and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4fce6877-04cf-452b-b305-5a5c2ca74703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 20580\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>folder</th><th>filename</th><th>width</th><th>height</th><th>dog_list</th></tr>\n",
       "<tr><td>02110627</td><td>n02110627_12938</td><td>200</td><td>300</td><td>[affenpinscher,0,...</td></tr>\n",
       "<tr><td>02093754</td><td>n02093754_1148</td><td>500</td><td>378</td><td>[Border_terrier,7...</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+---------------+-----+------+--------------------+\n",
       "|  folder|       filename|width|height|            dog_list|\n",
       "+--------+---------------+-----+------+--------------------+\n",
       "|02110627|n02110627_12938|  200|   300|[affenpinscher,0,...|\n",
       "|02093754| n02093754_1148|  500|   378|[Border_terrier,7...|\n",
       "+--------+---------------+-----+------+--------------------+"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reviewing the data\n",
    "print(f'Total rows: {clean_dog_df.count()}')\n",
    "clean_dog_df.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec316b04-fb1b-4a78-968b-9f7e3da7936d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 1106\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>_c0</th></tr>\n",
       "<tr><td>02085620</td></tr>\n",
       "<tr><td>02085782</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+\n",
       "|     _c0|\n",
       "+--------+\n",
       "|02085620|\n",
       "|02085782|\n",
       "+--------+"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing the valid_folders_df\n",
    "valid_folders_df = spark.read.csv('data-sources/annotations-validation.csv')\n",
    "\n",
    "print(f'Total rows: {valid_folders_df.count()}')\n",
    "valid_folders_df.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9147fb67-adbb-4742-abb5-6910aeac7835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before: 20580\n",
      "After: 19956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename the column in valid_folders_df\n",
    "valid_folders_df = valid_folders_df.withColumnRenamed('_c0', 'folder')\n",
    "\n",
    "# Count the number of rows in clean_dog_df\n",
    "clean_dog_count = clean_dog_df.count()\n",
    "\n",
    "# Join the DataFrames\n",
    "joined_df = clean_dog_df.join(F.broadcast(valid_folders_df), \"folder\")\n",
    "\n",
    "# Compare the number of rows remaining\n",
    "joined_count = joined_df.count()\n",
    "print(f'''\n",
    "Before: {clean_dog_count}\n",
    "After: {joined_count}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27fb23f-1c77-407a-8ac7-165ba3d4d00a",
   "metadata": {},
   "source": [
    "## Ex. 7 - Examining invalid rows\n",
    "\n",
    "You've successfully filtered out the rows using a join, but sometimes you'd like to examine the data that is invalid. This data can be stored for later processing or for troubleshooting your data sources. You want to find the difference between two DataFrames and store the invalid rows.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Determine the row counts for each DataFrame.\n",
    "2. Create a DataFrame containing only the invalid rows.\n",
    "3. Validate the count of the new DataFrame is as expected.\n",
    "4. Determine the number of distinct folder rows removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ce670f1-5b09-40dc-8671-1617a257914f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clean_dog_df: 20580\n",
      "Joined_df   : 19956 \n",
      "Invalid_df  : 624\n",
      "--------------------------------------\n",
      "Distinct invalid folders found: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Determine the row counts for each DataFrame\n",
    "clean_dog_count = clean_dog_df.count()\n",
    "joined_count = joined_df.count()\n",
    "\n",
    "# Create a DataFrame containing the invalid rows\n",
    "invalid_df = clean_dog_df.join(F.broadcast(joined_df), on='folder', how='left_anti')\n",
    "\n",
    "# Validate the count of the new DataFrame is as expected\n",
    "invalid_count = invalid_df.count()\n",
    "\n",
    "# Determine the number of distinct folder rows removed\n",
    "invalid_folder_count = invalid_df.select('folder').distinct().count()\n",
    "\n",
    "# Print the results\n",
    "print(f'''\n",
    "Clean_dog_df: {clean_dog_count}\n",
    "Joined_df   : {joined_count} \n",
    "Invalid_df  : {invalid_count}\n",
    "--------------------------------------\n",
    "Distinct invalid folders found: {invalid_folder_count}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9046cda-c472-474f-9c33-a8e402eed16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>folder</th><th>filename</th><th>width</th><th>height</th><th>dog_list</th></tr>\n",
       "<tr><td>%s</td><td>%s</td><td>800</td><td>600</td><td>[Shetland_sheepdo...</td></tr>\n",
       "<tr><td>%s</td><td>%s</td><td>333</td><td>500</td><td>[French_bulldog,4...</td></tr>\n",
       "<tr><td>%s</td><td>%s</td><td>375</td><td>500</td><td>[Shetland_sheepdo...</td></tr>\n",
       "<tr><td>%s</td><td>%s</td><td>500</td><td>355</td><td>[French_bulldog,1...</td></tr>\n",
       "<tr><td>%s</td><td>%s</td><td>160</td><td>180</td><td>[Shetland_sheepdo...</td></tr>\n",
       "<tr><td>%s</td><td>%s</td><td>191</td><td>284</td><td>[French_bulldog,1...</td></tr>\n",
       "<tr><td>%s</td><td>%s</td><td>209</td><td>150</td><td>[English_foxhound...</td></tr>\n",
       "<tr><td>%s</td><td>%s</td><td>210</td><td>173</td><td>[Shetland_sheepdo...</td></tr>\n",
       "<tr><td>%s</td><td>%s</td><td>500</td><td>456</td><td>[English_foxhound...</td></tr>\n",
       "<tr><td>%s</td><td>%s</td><td>300</td><td>271</td><td>[English_foxhound...</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------+--------+-----+------+--------------------+\n",
       "|folder|filename|width|height|            dog_list|\n",
       "+------+--------+-----+------+--------------------+\n",
       "|    %s|      %s|  800|   600|[Shetland_sheepdo...|\n",
       "|    %s|      %s|  333|   500|[French_bulldog,4...|\n",
       "|    %s|      %s|  375|   500|[Shetland_sheepdo...|\n",
       "|    %s|      %s|  500|   355|[French_bulldog,1...|\n",
       "|    %s|      %s|  160|   180|[Shetland_sheepdo...|\n",
       "|    %s|      %s|  191|   284|[French_bulldog,1...|\n",
       "|    %s|      %s|  209|   150|[English_foxhound...|\n",
       "|    %s|      %s|  210|   173|[Shetland_sheepdo...|\n",
       "|    %s|      %s|  500|   456|[English_foxhound...|\n",
       "|    %s|      %s|  300|   271|[English_foxhound...|\n",
       "+------+--------+-----+------+--------------------+"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reviewing what is the unique folder value not found\n",
    "invalid_df.limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99db4ca0-5531-48fe-975b-0aabb7d38206",
   "metadata": {},
   "source": [
    "## Ex. 8 - Dog parsing\n",
    "\n",
    "You've done a considerable amount of cleanup on the initial dataset, but now need to analyze the data a bit deeper. There are several questions that have now come up about the type of dogs seen in an image and some details regarding the images. You realize that to answer these questions, you need to process the data into a specific type. Before you can use it, you'll need to create a schema / type to represent the dog details.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Select the column representing the dog details from the DataFrame and show the first 10 un-truncated rows.\n",
    "2. Create a new schema as you've done before, using `breed`, `start_x`, `start_y`, `end_x`, and `end_y` as the names. Make sure to specify the proper data types for each field in the schema (any number value is an integer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7ed613e-f169-4a80-9387-04b626eace59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|dog_list                          |\n",
      "+----------------------------------+\n",
      "|[affenpinscher,0,9,173,298]       |\n",
      "|[Border_terrier,73,127,341,335]   |\n",
      "|[kuvasz,0,0,499,327]              |\n",
      "|[Great_Pyrenees,124,225,403,374]  |\n",
      "|[schipperke,146,29,416,309]       |\n",
      "|[groenendael,168,0,469,374]       |\n",
      "|[Bedlington_terrier,10,12,462,332]|\n",
      "|[Lhasa,39,1,499,373]              |\n",
      "|[Kerry_blue_terrier,17,16,300,482]|\n",
      "|[vizsla,112,93,276,236]           |\n",
      "+----------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the dog details and show 10 untruncated rows\n",
    "joined_df.select('dog_list').show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "503eec64-bd3c-47d7-bbae-a55296992cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a schema type for the details in the dog list\n",
    "DogType = StructType([\n",
    "\tStructField(\"breed\", StringType(), False),\n",
    "    StructField(\"start_x\", IntegerType(), False),\n",
    "    StructField(\"start_y\", IntegerType(), False),\n",
    "    StructField(\"end_x\", IntegerType(), False),\n",
    "    StructField(\"end_y\", IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ed10b9-6c2c-4907-9695-25b29180fd10",
   "metadata": {},
   "source": [
    "## Ex. 9 - Per image count\n",
    "\n",
    "Your next task in building a data pipeline for this dataset is to create a few analysis oriented columns. You've been asked to calculate the number of dogs found in each image based on your `dog_list` column created earlier. You have also created the `DogType` which will allow better parsing of the data within some of the data columns.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Create a Python function to split each entry in `dog_list` to its appropriate parts. Make sure to convert any strings into the appropriate types or the `DogType` will not parse correctly.\n",
    "2. Create a UDF using the above function.\n",
    "3. Use the UDF to create a new column called `dogs`. Drop the previous column in the same command.\n",
    "4. Show the number of dogs in the new column for the first 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "74eae1b4-4971-4252-b2f3-25b7b932560a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-----+------+-------------------------------------+\n",
      "|folder  |filename       |width|height|dogs                                 |\n",
      "+--------+---------------+-----+------+-------------------------------------+\n",
      "|02110627|n02110627_12938|200  |300   |[{affenpinscher, 0, 9, 173, 298}]    |\n",
      "|02093754|n02093754_1148 |500  |378   |[{Border_terrier, 73, 127, 341, 335}]|\n",
      "+--------+---------------+-----+------+-------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a function to return the number and type of dogs as a tuple\n",
    "def dogParse(doglist):\n",
    "    '''Return a tupla with breed, start_x, start_y, end_x, and end_y'''\n",
    "    dogs = []\n",
    "    for dog in doglist:\n",
    "        (breed, start_x, start_y, end_x, end_y) = dog.split(',')\n",
    "        dogs.append((breed, int(start_x), int(start_y), int(end_x), int(end_y)))\n",
    "    return dogs\n",
    "\n",
    "# Create a UDF\n",
    "udfDogParse = F.udf(dogParse, ArrayType(DogType))\n",
    "\n",
    "# Use the UDF to list of dogs and drop the old column\n",
    "clean_dog_df_parsed = clean_dog_df.withColumn('dogs', udfDogParse('dog_list')).drop('dog_list')\n",
    "clean_dog_df_parsed.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fec93e31-ae68-4273-9954-401923ec77a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|size(dogs)|\n",
      "+----------+\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the number of dogs in the first 10 rows\n",
    "clean_dog_df_parsed.select(F.size('dogs')).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166c363-bfeb-42da-9b05-c9e3177d51fd",
   "metadata": {},
   "source": [
    "## Ex. 10 - Percentage dog pixels\n",
    "The final task for parsing the dog annotation data is to determine the percentage of pixels in each image that represents a dog (or dogs). You'll need to use the various techniques you've learned in this course to help calculate this information and add it as columns for later analysis.\n",
    "\n",
    "To calculate the percentage of pixels, first calculate the total number of pixels representing each dog then sum them for the image. You can calculate the bounding box with the formula:\n",
    "\n",
    "`(Xend - Xstart) * (Yend - Ystart)`\n",
    "\n",
    "**NOTE**: You can ignore the possibility of overlapping bounding boxes in this instance.\n",
    "\n",
    "For the percentage, calculate the total number of \"dog\" pixels divided by the total size of the image, multiplied by 100.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Define a Python function to take a list of tuples (the dog objects) and calculate the total number of \"dog\" pixels per image.\n",
    "2. Create a UDF of the function and use it to create a new column called `'dog_pixels'` on the DataFrame.\n",
    "3. Create another column, `'dog_percent'`, representing the percentage of `'dog_pixels'` in the image. Make sure this is between 0-100%. Use the string name of the column alone (ie, \"columnname\" rather than df.columnname).\n",
    "4. Show the first 10 rows with more than 60% `'dog_pixels'` in the image. Use a SQL style string for this (ie, 'columnname > ____')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "41cb37fe-eefd-4472-a578-dfbdff795f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-----+------+-------------------------------------+\n",
      "|folder  |filename       |width|height|dogs                                 |\n",
      "+--------+---------------+-----+------+-------------------------------------+\n",
      "|02110627|n02110627_12938|200  |300   |[{affenpinscher, 0, 9, 173, 298}]    |\n",
      "|02093754|n02093754_1148 |500  |378   |[{Border_terrier, 73, 127, 341, 335}]|\n",
      "+--------+---------------+-----+------+-------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Review the data\n",
    "clean_dog_df_parsed.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cde9a3af-04a7-44e6-9895-51eac09f120a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-----+------+-------------------------------------+----------+\n",
      "|folder  |filename       |width|height|dogs                                 |dog_pixels|\n",
      "+--------+---------------+-----+------+-------------------------------------+----------+\n",
      "|02110627|n02110627_12938|200  |300   |[{affenpinscher, 0, 9, 173, 298}]    |49997     |\n",
      "|02093754|n02093754_1148 |500  |378   |[{Border_terrier, 73, 127, 341, 335}]|55744     |\n",
      "+--------+---------------+-----+------+-------------------------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a UDF to determine the number of pixels per image\n",
    "def dogPixelCount(doglist):\n",
    "    '''Return the total pixels per dog image'''\n",
    "    totalpixels = 0\n",
    "    for dog in doglist:\n",
    "        totalpixels += (dog[3] - dog[1]) * (dog[4] - dog[2])\n",
    "    return totalpixels\n",
    "\n",
    "# Define a UDF for the pixel count\n",
    "udfDogPixelCount = F.udf(dogPixelCount, IntegerType())\n",
    "\n",
    "# Use UDF with Spark\n",
    "dog_pixels_df = clean_dog_df_parsed.withColumn('dog_pixels', udfDogPixelCount('dogs'))\n",
    "dog_pixels_df.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "13cea0d2-df27-4070-bd70-f168e580d322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-----+------+-------------------------------------+----------+------------------+\n",
      "|folder  |filename       |width|height|dogs                                 |dog_pixels|dog_percent       |\n",
      "+--------+---------------+-----+------+-------------------------------------+----------+------------------+\n",
      "|02110627|n02110627_12938|200  |300   |[{affenpinscher, 0, 9, 173, 298}]    |49997     |83.32833333333333 |\n",
      "|02093754|n02093754_1148 |500  |378   |[{Border_terrier, 73, 127, 341, 335}]|55744     |29.494179894179894|\n",
      "+--------+---------------+-----+------+-------------------------------------+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a column representing the percentage of pixels\n",
    "dog_pixels_60_df = dog_pixels_df.withColumn(\n",
    "    'dog_percent', \n",
    "    (dog_pixels_df['dog_pixels'] / (\n",
    "        dog_pixels_df['width'].cast('Double') * dog_pixels_df['height'].cast('Double')\n",
    "    )) * 100)\n",
    "dog_pixels_60_df.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "25239f15-6732-422f-9a30-ec365a6846ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>folder</th><th>filename</th><th>width</th><th>height</th><th>dogs</th><th>dog_pixels</th><th>dog_percent</th></tr>\n",
       "<tr><td>02110627</td><td>n02110627_12938</td><td>200</td><td>300</td><td>[{affenpinscher, ...</td><td>49997</td><td>83.32833333333333</td></tr>\n",
       "<tr><td>02104029</td><td>n02104029_63</td><td>500</td><td>375</td><td>[{kuvasz, 0, 0, 4...</td><td>163173</td><td>87.0256</td></tr>\n",
       "<tr><td>02105056</td><td>n02105056_2834</td><td>500</td><td>375</td><td>[{groenendael, 16...</td><td>112574</td><td>60.03946666666666</td></tr>\n",
       "<tr><td>02093647</td><td>n02093647_541</td><td>500</td><td>333</td><td>[{Bedlington_terr...</td><td>144640</td><td>86.87087087087087</td></tr>\n",
       "<tr><td>02098413</td><td>n02098413_1355</td><td>500</td><td>375</td><td>[{Lhasa, 39, 1, 4...</td><td>171120</td><td>91.264</td></tr>\n",
       "<tr><td>02093859</td><td>n02093859_2309</td><td>330</td><td>500</td><td>[{Kerry_blue_terr...</td><td>131878</td><td>79.92606060606062</td></tr>\n",
       "<tr><td>02109961</td><td>n02109961_1017</td><td>475</td><td>500</td><td>[{Eskimo_dog, 43,...</td><td>189189</td><td>79.65852631578947</td></tr>\n",
       "<tr><td>02108000</td><td>n02108000_3491</td><td>600</td><td>450</td><td>[{EntleBucher, 30...</td><td>168667</td><td>62.46925925925926</td></tr>\n",
       "<tr><td>02085782</td><td>n02085782_1731</td><td>600</td><td>449</td><td>[{Japanese_spanie...</td><td>250125</td><td>92.84521158129176</td></tr>\n",
       "<tr><td>02110185</td><td>n02110185_2736</td><td>259</td><td>500</td><td>[{Siberian_husky,...</td><td>113088</td><td>87.32664092664093</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+---------------+-----+------+--------------------+----------+-----------------+\n",
       "|  folder|       filename|width|height|                dogs|dog_pixels|      dog_percent|\n",
       "+--------+---------------+-----+------+--------------------+----------+-----------------+\n",
       "|02110627|n02110627_12938|  200|   300|[{affenpinscher, ...|     49997|83.32833333333333|\n",
       "|02104029|   n02104029_63|  500|   375|[{kuvasz, 0, 0, 4...|    163173|          87.0256|\n",
       "|02105056| n02105056_2834|  500|   375|[{groenendael, 16...|    112574|60.03946666666666|\n",
       "|02093647|  n02093647_541|  500|   333|[{Bedlington_terr...|    144640|86.87087087087087|\n",
       "|02098413| n02098413_1355|  500|   375|[{Lhasa, 39, 1, 4...|    171120|           91.264|\n",
       "|02093859| n02093859_2309|  330|   500|[{Kerry_blue_terr...|    131878|79.92606060606062|\n",
       "|02109961| n02109961_1017|  475|   500|[{Eskimo_dog, 43,...|    189189|79.65852631578947|\n",
       "|02108000| n02108000_3491|  600|   450|[{EntleBucher, 30...|    168667|62.46925925925926|\n",
       "|02085782| n02085782_1731|  600|   449|[{Japanese_spanie...|    250125|92.84521158129176|\n",
       "|02110185| n02110185_2736|  259|   500|[{Siberian_husky,...|    113088|87.32664092664093|\n",
       "+--------+---------------+-----+------+--------------------+----------+-----------------+"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the first 10 annotations with more than 60% dog\n",
    "dog_pixels_60_df.filter('dog_percent > 60').limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84744cb-27ee-40bb-a5c3-a8b24d7342c0",
   "metadata": {},
   "source": [
    "## Close session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a35b740-9e99-4e09-8ccc-81e0ea59e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382aa0d6-d5f7-4180-a803-f58b165acd30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
