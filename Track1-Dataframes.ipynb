{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da98c1d6-c41f-4d75-980e-cf7f6540711c",
   "metadata": {},
   "source": [
    "# DataFrame details\n",
    "\n",
    "A review of DataFrame fundamentals and the importance of data cleaning.\n",
    "\n",
    "## Preparing the environment\n",
    "\n",
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84770b51-2d57-4b9d-b8ff-bc5b8773783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (_parse_datatype_string, StructType, StructField,\n",
    "                               DoubleType, IntegerType, StringType)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3751f7-cc7f-4c27-9055-d76887924963",
   "metadata": {},
   "source": [
    "### Connect to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d00fd800-7416-4120-b336-c915d9797bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# eval DataFrame in notebooks\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffd0fb21-d1fe-49c0-abf0-15b3702f5313",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea505934-a70b-4017-9af1-e33c8399adb0",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b83775-1081-484a-a393-9944be747d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date (MM/DD/YYYY): date (nullable = true)\n",
      " |-- Flight Number: integer (nullable = true)\n",
      " |-- Destination Airport: string (nullable = true)\n",
      " |-- Actual elapsed time (Minutes): integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Date (MM/DD/YYYY)</th><th>Flight Number</th><th>Destination Airport</th><th>Actual elapsed time (Minutes)</th></tr>\n",
       "<tr><td>2014-01-01</td><td>5</td><td>HNL</td><td>519</td></tr>\n",
       "<tr><td>2014-01-01</td><td>7</td><td>OGG</td><td>505</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+-------------+-------------------+-----------------------------+\n",
       "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
       "+-----------------+-------------+-------------------+-----------------------------+\n",
       "|       2014-01-01|            5|                HNL|                          519|\n",
       "|       2014-01-01|            7|                OGG|                          505|\n",
       "+-----------------+-------------+-------------------+-----------------------------+"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_2014 = spark.read.csv('data-sources/AA_DFW_2014_Departures_Short.csv.gz', header=True, inferSchema=True)\n",
    "\n",
    "# cast to date\n",
    "flights_2014 = flights_2014.withColumn(\"Date (MM/DD/YYYY)\", \n",
    "                                       F.to_date(flights_2014[\"Date (MM/DD/YYYY)\"], \"MM/dd/yyyy\"))\n",
    "\n",
    "flights_2014.createOrReplaceTempView(\"flights_2014\")\n",
    "flights_2014.printSchema()\n",
    "flights_2014.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b689046-34a9-4503-8e59-97f61b064c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date (MM/DD/YYYY): date (nullable = true)\n",
      " |-- Flight Number: integer (nullable = true)\n",
      " |-- Destination Airport: string (nullable = true)\n",
      " |-- Actual elapsed time (Minutes): integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Date (MM/DD/YYYY)</th><th>Flight Number</th><th>Destination Airport</th><th>Actual elapsed time (Minutes)</th></tr>\n",
       "<tr><td>2015-01-01</td><td>5</td><td>HNL</td><td>526</td></tr>\n",
       "<tr><td>2015-01-01</td><td>7</td><td>OGG</td><td>517</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+-------------+-------------------+-----------------------------+\n",
       "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
       "+-----------------+-------------+-------------------+-----------------------------+\n",
       "|       2015-01-01|            5|                HNL|                          526|\n",
       "|       2015-01-01|            7|                OGG|                          517|\n",
       "+-----------------+-------------+-------------------+-----------------------------+"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_2015 = spark.read.csv('data-sources/AA_DFW_2015_Departures_Short.csv.gz', header=True, inferSchema=True)\n",
    "\n",
    "# cast to date\n",
    "flights_2015 = flights_2015.withColumn(\"Date (MM/DD/YYYY)\", \n",
    "                                       F.to_date(flights_2015[\"Date (MM/DD/YYYY)\"], \"MM/dd/yyyy\"))\n",
    "\n",
    "flights_2015.createOrReplaceTempView(\"flights_2015\")\n",
    "flights_2015.printSchema()\n",
    "flights_2015.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96fcc16c-b1ea-4219-9833-ad050a1d25d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date (MM/DD/YYYY): date (nullable = true)\n",
      " |-- Flight Number: integer (nullable = true)\n",
      " |-- Destination Airport: string (nullable = true)\n",
      " |-- Actual elapsed time (Minutes): integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Date (MM/DD/YYYY)</th><th>Flight Number</th><th>Destination Airport</th><th>Actual elapsed time (Minutes)</th></tr>\n",
       "<tr><td>2017-01-01</td><td>5</td><td>HNL</td><td>537</td></tr>\n",
       "<tr><td>2017-01-01</td><td>7</td><td>OGG</td><td>498</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+-------------+-------------------+-----------------------------+\n",
       "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
       "+-----------------+-------------+-------------------+-----------------------------+\n",
       "|       2017-01-01|            5|                HNL|                          537|\n",
       "|       2017-01-01|            7|                OGG|                          498|\n",
       "+-----------------+-------------+-------------------+-----------------------------+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_2017 = spark.read.csv('data-sources/AA_DFW_2017_Departures_Short.csv.gz', header=True, inferSchema=True)\n",
    "\n",
    "# cast to date\n",
    "flights_2017 = flights_2017.withColumn(\"Date (MM/DD/YYYY)\", \n",
    "                                       F.to_date(flights_2017[\"Date (MM/DD/YYYY)\"], \"MM/dd/yyyy\"))\n",
    "flights_2017.createOrReplaceTempView(\"flights_2017\")\n",
    "flights_2017.printSchema()\n",
    "flights_2017.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfbb6415-4ff7-417a-99ea-2851ce4ac093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date (MM/DD/YYYY): date (nullable = true)\n",
      " |-- Flight Number: integer (nullable = true)\n",
      " |-- Destination Airport: string (nullable = true)\n",
      " |-- Actual elapsed time (Minutes): integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Date (MM/DD/YYYY)</th><th>Flight Number</th><th>Destination Airport</th><th>Actual elapsed time (Minutes)</th></tr>\n",
       "<tr><td>2018-01-01</td><td>5</td><td>HNL</td><td>498</td></tr>\n",
       "<tr><td>2018-01-01</td><td>7</td><td>OGG</td><td>501</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+-------------+-------------------+-----------------------------+\n",
       "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
       "+-----------------+-------------+-------------------+-----------------------------+\n",
       "|       2018-01-01|            5|                HNL|                          498|\n",
       "|       2018-01-01|            7|                OGG|                          501|\n",
       "+-----------------+-------------+-------------------+-----------------------------+"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_2018 = spark.read.csv('data-sources/AA_DFW_2018_Departures_Short.csv.gz', header=True, inferSchema=True)\n",
    "\n",
    "# cast to date\n",
    "flights_2018 = flights_2018.withColumn(\"Date (MM/DD/YYYY)\", \n",
    "                                       F.to_date(flights_2018[\"Date (MM/DD/YYYY)\"], \"MM/dd/yyyy\"))\n",
    "flights_2018.createOrReplaceTempView(\"flights_2018\")\n",
    "flights_2018.printSchema()\n",
    "flights_2018.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da08c587-0bbe-4a65-955d-f35c931ff7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DATE: date (nullable = true)\n",
      " |-- TITLE: string (nullable = true)\n",
      " |-- VOTER_NAME: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>DATE</th><th>TITLE</th><th>VOTER_NAME</th></tr>\n",
       "<tr><td>2017-02-08</td><td>Councilmember</td><td>Jennifer S. Gates</td></tr>\n",
       "<tr><td>2017-02-08</td><td>Councilmember</td><td>Philip T. Kingston</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+-------------+------------------+\n",
       "|      DATE|        TITLE|        VOTER_NAME|\n",
       "+----------+-------------+------------------+\n",
       "|2017-02-08|Councilmember| Jennifer S. Gates|\n",
       "|2017-02-08|Councilmember|Philip T. Kingston|\n",
       "+----------+-------------+------------------+"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dallas_electors = spark.read.csv('data-sources/DallasCouncilVoters.csv.gz', header=True, inferSchema=True)\n",
    "\n",
    "# cast to date\n",
    "dallas_electors = dallas_electors.withColumn(\"DATE\", F.to_date(dallas_electors[\"DATE\"], \"MM/dd/yyyy\"))\n",
    "\n",
    "dallas_electors.createOrReplaceTempView(\"dallas_electors\")\n",
    "dallas_electors.printSchema()\n",
    "dallas_electors.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28b876ad-3103-481b-94c6-a84c80cc9c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DATE: date (nullable = true)\n",
      " |-- AGENDA_ITEM_NUMBER: string (nullable = true)\n",
      " |-- ITEM_TYPE: string (nullable = true)\n",
      " |-- DISTRICT: string (nullable = true)\n",
      " |-- TITLE: string (nullable = true)\n",
      " |-- VOTER NAME: string (nullable = true)\n",
      " |-- VOTE CAST: string (nullable = true)\n",
      " |-- FINAL ACTION TAKEN: string (nullable = true)\n",
      " |-- AGENDA ITEM DESCRIPTION: string (nullable = true)\n",
      " |-- AGENDA_ID: string (nullable = true)\n",
      " |-- VOTE_ID: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>DATE</th><th>AGENDA_ITEM_NUMBER</th><th>ITEM_TYPE</th><th>DISTRICT</th><th>TITLE</th><th>VOTER NAME</th><th>VOTE CAST</th><th>FINAL ACTION TAKEN</th><th>AGENDA ITEM DESCRIPTION</th><th>AGENDA_ID</th><th>VOTE_ID</th></tr>\n",
       "<tr><td>2017-02-08</td><td>1</td><td>AGENDA</td><td>13</td><td>Councilmember</td><td>Jennifer S. Gates</td><td>N/A</td><td>NO ACTION NEEDED</td><td>Call to Order</td><td>020817__Special__1</td><td>020817__Special__...</td></tr>\n",
       "<tr><td>2017-02-08</td><td>1</td><td>AGENDA</td><td>14</td><td>Councilmember</td><td>Philip T. Kingston</td><td>N/A</td><td>NO ACTION NEEDED</td><td>Call to Order</td><td>020817__Special__1</td><td>020817__Special__...</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+------------------+---------+--------+-------------+------------------+---------+------------------+-----------------------+------------------+--------------------+\n",
       "|      DATE|AGENDA_ITEM_NUMBER|ITEM_TYPE|DISTRICT|        TITLE|        VOTER NAME|VOTE CAST|FINAL ACTION TAKEN|AGENDA ITEM DESCRIPTION|         AGENDA_ID|             VOTE_ID|\n",
       "+----------+------------------+---------+--------+-------------+------------------+---------+------------------+-----------------------+------------------+--------------------+\n",
       "|2017-02-08|                 1|   AGENDA|      13|Councilmember| Jennifer S. Gates|      N/A|  NO ACTION NEEDED|          Call to Order|020817__Special__1|020817__Special__...|\n",
       "|2017-02-08|                 1|   AGENDA|      14|Councilmember|Philip T. Kingston|      N/A|  NO ACTION NEEDED|          Call to Order|020817__Special__1|020817__Special__...|\n",
       "+----------+------------------+---------+--------+-------------+------------------+---------+------------------+-----------------------+------------------+--------------------+"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dallas_votes = spark.read.csv('data-sources/DallasCouncilVotes.csv.gz', header=True, inferSchema=True)\n",
    "\n",
    "# cast to date\n",
    "dallas_votes = dallas_votes.withColumn(\"DATE\", F.to_date(dallas_votes[\"DATE\"], \"MM/dd/yyyy\"))\n",
    "\n",
    "dallas_votes.createOrReplaceTempView(\"dallas_votes\")\n",
    "dallas_votes.printSchema()\n",
    "dallas_votes.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55e41a60-4db8-408e-83c1-e7e849396cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>name</th><th>age</th><th>city</th></tr>\n",
       "<tr><td>Amy Meyer</td><td>3</td><td>Kimberlyborough</td></tr>\n",
       "<tr><td>Amy Jones</td><td>10</td><td>Davidburgh</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+---+---------------+\n",
       "|     name|age|           city|\n",
       "+---------+---+---------------+\n",
       "|Amy Meyer|  3|Kimberlyborough|\n",
       "|Amy Jones| 10|     Davidburgh|\n",
       "+---------+---+---------------+"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people = spark.read.csv('data-sources/people_data_sample.csv', header=True, inferSchema=True)\n",
    "people.createOrReplaceTempView(\"people\")\n",
    "people.printSchema()\n",
    "people.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a44b0f1-acdf-4ac7-9010-9de1632dfc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FL_DATE: date (nullable = true)\n",
      " |-- OP_CARRIER: string (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: integer (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- ORIGIN_CITY_NAME: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- DEST_CITY_NAME: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: integer (nullable = true)\n",
      " |-- DEP_TIME: integer (nullable = true)\n",
      " |-- WHEELS_ON: integer (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- CRS_ARR_TIME: integer (nullable = true)\n",
      " |-- ARR_TIME: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>FL_DATE</th><th>OP_CARRIER</th><th>OP_CARRIER_FL_NUM</th><th>ORIGIN</th><th>ORIGIN_CITY_NAME</th><th>DEST</th><th>DEST_CITY_NAME</th><th>CRS_DEP_TIME</th><th>DEP_TIME</th><th>WHEELS_ON</th><th>TAXI_IN</th><th>CRS_ARR_TIME</th><th>ARR_TIME</th><th>CANCELLED</th><th>DISTANCE</th></tr>\n",
       "<tr><td>2000-01-01</td><td>DL</td><td>1451</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1115</td><td>1113</td><td>1343</td><td>5</td><td>1400</td><td>1348</td><td>0</td><td>946</td></tr>\n",
       "<tr><td>2000-01-01</td><td>DL</td><td>1479</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1315</td><td>1311</td><td>1536</td><td>7</td><td>1559</td><td>1543</td><td>0</td><td>946</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
       "|   FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|ORIGIN_CITY_NAME|DEST|DEST_CITY_NAME|CRS_DEP_TIME|DEP_TIME|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|CANCELLED|DISTANCE|\n",
       "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
       "|2000-01-01|        DL|             1451|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1115|    1113|     1343|      5|        1400|    1348|        0|     946|\n",
       "|2000-01-01|        DL|             1479|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1315|    1311|     1536|      7|        1559|    1543|        0|     946|\n",
       "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight = spark.read.parquet('data-sources/flight-time.parquet')\n",
    "flight.createOrReplaceTempView(\"flight\")\n",
    "flight.printSchema()\n",
    "flight.limit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9d4812-a693-4bca-ad36-d6f4f26cdc84",
   "metadata": {},
   "source": [
    "### Tables catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79396001-462c-4a29-af32-99de195527fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='dallas_electors', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='dallas_votes', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='flight', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='flights_2014', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='flights_2015', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='flights_2017', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='flights_2018', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='people', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fd6885-feea-4229-975b-8a4648568052",
   "metadata": {},
   "source": [
    "## Intro to data cleaning with Apache Spark\n",
    "\n",
    "### Example Spark Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb677e4e-1536-4485-882b-67bcb0d96d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>name</th><th>age</th><th>city</th></tr>\n",
       "<tr><td>Amy Meyer</td><td>3</td><td>Kimberlyborough</td></tr>\n",
       "<tr><td>Amy Jones</td><td>10</td><td>Davidburgh</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+---+---------------+\n",
       "|     name|age|           city|\n",
       "+---------+---+---------------+\n",
       "|Amy Meyer|  3|Kimberlyborough|\n",
       "|Amy Jones| 10|     Davidburgh|\n",
       "+---------+---+---------------+"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Schema definition\n",
    "peopleSchema = StructType([\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('age', IntegerType(), True),\n",
    "    StructField('city', StringType(), True)\n",
    "])\n",
    "\n",
    "# Read CSV file containing data\n",
    "people_df = (spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \",\")\n",
    "                  .schema(peopleSchema)\n",
    "                  .load(\"data-sources/people_data_sample.csv\", schema=peopleSchema))\n",
    "people_df.printSchema()\n",
    "people_df.limit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ca2613-c76f-4799-834f-d92a2fbdaf88",
   "metadata": {},
   "source": [
    "## Ex. 1 - Defining a schema\n",
    "\n",
    "Creating a defined schema helps with data quality and import performance. As mentioned, we'll create a simple schema to read in the following columns:\n",
    "- Name\n",
    "- Age\n",
    "- City\n",
    "\n",
    "The `Name` and `City` columns are `StringType()` and the `Age` column is an `IntegerType()`.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Import `*` from the `pyspark.sql.types` library. (A;ready done)\n",
    "2. Define a new schema using the `StructType` method.\n",
    "3. Define a `StructField` for `name`, `age`, and `city`. Each field should correspond to the correct datatype and not be nullable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58826116-401a-4709-8c34-48ed79f2447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new schema using the StructType method\n",
    "people_schema = StructType([\n",
    "    # Define a StructField for each field\n",
    "    StructField('name', StringType(), False),\n",
    "    StructField('age', IntegerType(), False),\n",
    "    StructField('city', StringType(), False),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0376af11-1da9-4940-b704-368adc18a3c2",
   "metadata": {},
   "source": [
    "## Immutability and lazy processing\n",
    "\n",
    "### Immutability Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11d082e1-835a-42db-a752-0af7a290aa03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DATE: date (nullable = true)\n",
      " |-- TITLE: string (nullable = true)\n",
      " |-- VOTER_NAME: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>DATE</th><th>TITLE</th><th>VOTER_NAME</th></tr>\n",
       "<tr><td>2017-02-08</td><td>Councilmember</td><td>Jennifer S. Gates</td></tr>\n",
       "<tr><td>2017-02-08</td><td>Councilmember</td><td>Philip T. Kingston</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+-------------+------------------+\n",
       "|      DATE|        TITLE|        VOTER_NAME|\n",
       "+----------+-------------+------------------+\n",
       "|2017-02-08|Councilmember| Jennifer S. Gates|\n",
       "|2017-02-08|Councilmember|Philip T. Kingston|\n",
       "+----------+-------------+------------------+"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reviewing the data frame:\n",
    "voter_df = dallas_electors.select(\"*\")\n",
    "voter_df.printSchema()\n",
    "voter_df.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c292b68-ebfb-49d2-bd4f-e50765536b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>DATE</th><th>TITLE</th><th>VOTER_NAME</th><th>GRAD_YEAR</th></tr>\n",
       "<tr><td>2017-02-08</td><td>Councilmember</td><td>Jennifer S. Gates</td><td>2027</td></tr>\n",
       "<tr><td>2017-02-08</td><td>Councilmember</td><td>Philip T. Kingston</td><td>2027</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+-------------+------------------+---------+\n",
       "|      DATE|        TITLE|        VOTER_NAME|GRAD_YEAR|\n",
       "+----------+-------------+------------------+---------+\n",
       "|2017-02-08|Councilmember| Jennifer S. Gates|     2027|\n",
       "|2017-02-08|Councilmember|Philip T. Kingston|     2027|\n",
       "+----------+-------------+------------------+---------+"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voter_df = voter_df.withColumn('GRAD_YEAR', F.year(voter_df.DATE) + 10)\n",
    "voter_df.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5339287-035d-4f95-a889-f648d8d3f40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DATE: date (nullable = true)\n",
      " |-- TITLE: string (nullable = true)\n",
      " |-- VOTER_NAME: string (nullable = true)\n",
      " |-- GRAD_YEAR: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e0230d-ccf2-4111-8ec7-411e222a1d8b",
   "metadata": {},
   "source": [
    "## Ex. 2 - Using lazy processing\n",
    "\n",
    "Lazy processing operations will usually return in about the same amount of time regardless of the actual quantity of data. Remember that this is due to Spark not performing any transformations until an action is requested.\n",
    "\n",
    "For this exercise, we'll be defining a Data Frame (`aa_dfw_df`) and add a couple transformations. Note the amount of time required for the transformations to complete when defined vs when the data is actually queried. These differences may be short, but they will be noticeable. When working with a full Spark cluster with larger quantities of data the difference will be more apparent.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Load the Data Frame.\n",
    "2. Add the transformation for `F.lower()` to the Destination Airport column.\n",
    "3. Drop the `Destination Airport` column from the Data Frame `aa_dfw_df`. Note the time for these operations to complete.\n",
    "4. Show the Data Frame, noting the time difference for this action to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "652afe05-892a-4e31-8d70-6519424de532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Date (MM/DD/YYYY)</th><th>Flight Number</th><th>Destination Airport</th><th>Actual elapsed time (Minutes)</th></tr>\n",
       "<tr><td>01/01/2017</td><td>0005</td><td>HNL</td><td>537</td></tr>\n",
       "<tr><td>01/01/2017</td><td>0007</td><td>OGG</td><td>498</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+-------------+-------------------+-----------------------------+\n",
       "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
       "+-----------------+-------------+-------------------+-----------------------------+\n",
       "|       01/01/2017|         0005|                HNL|                          537|\n",
       "|       01/01/2017|         0007|                OGG|                          498|\n",
       "+-----------------+-------------+-------------------+-----------------------------+"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "aa_dfw_df = (spark.read.format('csv').options(Header=True)\n",
    "                       .load('data-sources/AA_DFW_2017_Departures_Short.csv.gz'))\n",
    "aa_dfw_df.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b92a773-a273-4f4e-b7a2-d6ac59731658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Date (MM/DD/YYYY)</th><th>Flight Number</th><th>Actual elapsed time (Minutes)</th><th>Airport</th></tr>\n",
       "<tr><td>01/01/2017</td><td>0005</td><td>537</td><td>hnl</td></tr>\n",
       "<tr><td>01/01/2017</td><td>0007</td><td>498</td><td>ogg</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+-------------+-----------------------------+-------+\n",
       "|Date (MM/DD/YYYY)|Flight Number|Actual elapsed time (Minutes)|Airport|\n",
       "+-----------------+-------------+-----------------------------+-------+\n",
       "|       01/01/2017|         0005|                          537|    hnl|\n",
       "|       01/01/2017|         0007|                          498|    ogg|\n",
       "+-----------------+-------------+-----------------------------+-------+"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the airport column using the F.lower() method\n",
    "aa_dfw_df = aa_dfw_df.withColumn('Airport', F.lower(aa_dfw_df['Destination Airport']))\n",
    "\n",
    "# Drop the Destination Airport column\n",
    "aa_dfw_df = aa_dfw_df.drop(aa_dfw_df['Destination Airport'])\n",
    "\n",
    "# Show the DataFrame\n",
    "aa_dfw_df.limit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c42b232-274d-4cea-8c12-80481b7fd877",
   "metadata": {},
   "source": [
    "## Understanding Parquet\n",
    "\n",
    "### Working with Parquet - Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50fa2563-dd15-413a-a838-c1f60ccd5ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>FL_DATE</th><th>OP_CARRIER</th><th>OP_CARRIER_FL_NUM</th><th>ORIGIN</th><th>ORIGIN_CITY_NAME</th><th>DEST</th><th>DEST_CITY_NAME</th><th>CRS_DEP_TIME</th><th>DEP_TIME</th><th>WHEELS_ON</th><th>TAXI_IN</th><th>CRS_ARR_TIME</th><th>ARR_TIME</th><th>CANCELLED</th><th>DISTANCE</th></tr>\n",
       "<tr><td>2000-01-01</td><td>DL</td><td>1451</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1115</td><td>1113</td><td>1343</td><td>5</td><td>1400</td><td>1348</td><td>0</td><td>946</td></tr>\n",
       "<tr><td>2000-01-01</td><td>DL</td><td>1479</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1315</td><td>1311</td><td>1536</td><td>7</td><td>1559</td><td>1543</td><td>0</td><td>946</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
       "|   FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|ORIGIN_CITY_NAME|DEST|DEST_CITY_NAME|CRS_DEP_TIME|DEP_TIME|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|CANCELLED|DISTANCE|\n",
       "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
       "|2000-01-01|        DL|             1451|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1115|    1113|     1343|      5|        1400|    1348|        0|     946|\n",
       "|2000-01-01|        DL|             1479|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1315|    1311|     1536|      7|        1559|    1543|        0|     946|\n",
       "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'data-sources/flight-time.parquet'\n",
    "\n",
    "# Reading Parquet files - method 1\n",
    "df = spark.read.format('parquet').load(file_path)\n",
    "df.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "451791bf-a62f-4d98-9e4e-461f266aad9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>FL_DATE</th><th>OP_CARRIER</th><th>OP_CARRIER_FL_NUM</th><th>ORIGIN</th><th>ORIGIN_CITY_NAME</th><th>DEST</th><th>DEST_CITY_NAME</th><th>CRS_DEP_TIME</th><th>DEP_TIME</th><th>WHEELS_ON</th><th>TAXI_IN</th><th>CRS_ARR_TIME</th><th>ARR_TIME</th><th>CANCELLED</th><th>DISTANCE</th></tr>\n",
       "<tr><td>2000-01-01</td><td>DL</td><td>1451</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1115</td><td>1113</td><td>1343</td><td>5</td><td>1400</td><td>1348</td><td>0</td><td>946</td></tr>\n",
       "<tr><td>2000-01-01</td><td>DL</td><td>1479</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1315</td><td>1311</td><td>1536</td><td>7</td><td>1559</td><td>1543</td><td>0</td><td>946</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
       "|   FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|ORIGIN_CITY_NAME|DEST|DEST_CITY_NAME|CRS_DEP_TIME|DEP_TIME|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|CANCELLED|DISTANCE|\n",
       "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
       "|2000-01-01|        DL|             1451|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1115|    1113|     1343|      5|        1400|    1348|        0|     946|\n",
       "|2000-01-01|        DL|             1479|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1315|    1311|     1536|      7|        1559|    1543|        0|     946|\n",
       "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading Parquet files - method 2\n",
    "df = spark.read.parquet(file_path)\n",
    "df.limit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cea7be-ae0a-44e1-a84f-c49db8d8121a",
   "metadata": {},
   "source": [
    "### Working with Parquet - Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd419535-b482-48db-bf51-bd2c171e0553",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = 'output-files/flight-time.parquet'\n",
    "\n",
    "# Writing Parquet files - method 1\n",
    "df.write.format('parquet').save(output_file_path, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1dc64e8-760c-428b-93f3-67fd28d14108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing Parquet files - method 2\n",
    "df.write.parquet(output_file_path, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb06398f-7336-40c1-a3fd-ebc7eaf7b2f6",
   "metadata": {},
   "source": [
    "### Parquet and SQL\n",
    "\r\n",
    "Parquet as backing stores for SparkSQL operation00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8e425d8-a769-43a3-ad3f-956123efb57d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='dallas_electors', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='dallas_votes', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='flight', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='flights', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='flights_2014', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='flights_2015', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='flights_2017', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='flights_2018', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='people', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_file_path = 'output-files/flight-time.parquet'\n",
    "flight_df = spark.read.parquet(output_file_path)\n",
    "\n",
    "flight_df.createOrReplaceTempView('flights')\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "729f8937-6ab1-43c3-8ada-d7498de35778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>FL_DATE</th><th>OP_CARRIER</th><th>OP_CARRIER_FL_NUM</th><th>ORIGIN</th><th>ORIGIN_CITY_NAME</th><th>DEST</th><th>DEST_CITY_NAME</th><th>CRS_DEP_TIME</th><th>DEP_TIME</th><th>WHEELS_ON</th><th>TAXI_IN</th><th>CRS_ARR_TIME</th><th>ARR_TIME</th><th>CANCELLED</th><th>DISTANCE</th></tr>\n",
       "<tr><td>2000-01-01</td><td>DL</td><td>346</td><td>BTR</td><td>Baton Rouge, LA</td><td>ATL</td><td>Atlanta, GA</td><td>1740</td><td>1744</td><td>1957</td><td>9</td><td>2008</td><td>2006</td><td>0</td><td>449</td></tr>\n",
       "<tr><td>2000-01-01</td><td>DL</td><td>412</td><td>BTR</td><td>Baton Rouge, LA</td><td>ATL</td><td>Atlanta, GA</td><td>1345</td><td>1345</td><td>1552</td><td>9</td><td>1622</td><td>1601</td><td>0</td><td>449</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
       "|   FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|ORIGIN_CITY_NAME|DEST|DEST_CITY_NAME|CRS_DEP_TIME|DEP_TIME|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|CANCELLED|DISTANCE|\n",
       "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
       "|2000-01-01|        DL|              346|   BTR| Baton Rouge, LA| ATL|   Atlanta, GA|        1740|    1744|     1957|      9|        2008|    2006|        0|     449|\n",
       "|2000-01-01|        DL|              412|   BTR| Baton Rouge, LA| ATL|   Atlanta, GA|        1345|    1345|     1552|      9|        1622|    1601|        0|     449|\n",
       "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_flights_df = spark.sql('SELECT * FROM flights WHERE DISTANCE < 500')\n",
    "short_flights_df.limit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a84e5e0-a99c-4e1b-8f35-9796ec9b34aa",
   "metadata": {},
   "source": [
    "## Ex. 3 - Saving a DataFrame in Parquet format\n",
    "\n",
    "When working with Spark, you'll often start with CSV, JSON, or other data sources. This provides a lot of flexibility for the types of data to load, but it is not an optimal format for Spark. The Parquet format is a columnar data store, allowing Spark to use predicate pushdown. This means Spark will only process the data necessary to complete the operations you define versus reading the entire dataset. This gives Spark more flexibility in accessing the data and often drastically improves performance on large datasets.\n",
    "\n",
    "In this exercise, we're going to practice creating a new Parquet file and then process some data from it.\n",
    "**Instructions:**\n",
    "\n",
    "1. View the row count of `df1` and `df2`.\n",
    "2. Combine `df1` and `df2` in a new DataFrame named `df3` with the `union` method.\n",
    "3. Save `df3` to a parquet file named `AA_DFW_ALL.parquet`.\n",
    "4. Read the `AA_DFW_ALL.parquet` file and show the count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1dbd3b6a-3586-4a4d-b859-bef9063765a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 Count: 139358\n",
      "df2 Count: 119910\n",
      "Total    : 259268\n"
     ]
    }
   ],
   "source": [
    "# Setting the data\n",
    "df1 = flights_2017.select('*')\n",
    "df2 = flights_2018.select('*')\n",
    "\n",
    "# View the row count of df1 and df2\n",
    "print(\"df1 Count: %d\" % df1.count())\n",
    "print(\"df2 Count: %d\" % df2.count())\n",
    "\n",
    "# Combine the DataFrames into one\n",
    "df3 = df1.union(df2)\n",
    "\n",
    "# Save the df3 DataFrame in Parquet format\n",
    "df3.write.parquet('output-files/AA_DFW_ALL.parquet', mode='overwrite')\n",
    "\n",
    "# Read the Parquet file into a new DataFrame and run a count\n",
    "print(\"Total    :\", spark.read.parquet('output-files/AA_DFW_ALL.parquet').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6e9f9e-7d8d-43df-861e-b1fc28619d15",
   "metadata": {},
   "source": [
    "## Ex. 4 - SQL and Parquet\n",
    "\n",
    "Parquet files are perfect as a backing data store for SQL queries in Spark. While it is possible to run the same queries directly via Spark's Python functions, sometimes it's easier to run SQL queries alongside the Python options.\n",
    "\n",
    "For this example, we're going to read in the Parquet file we created in the last exercise and register it as a SQL table. Once registered, we'll run a quick query against the table (aka, the Parquet file).\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Import the `AA_DFW_ALL.parquet` file into `flights_df`.\n",
    "2. Use the `createOrReplaceTempView` method to alias the `flights` table.\n",
    "3. Run a Spark SQL query against the `flights` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3e5eda5-44b6-4935-8e76-5855c1550749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date (MM/DD/YYYY): date (nullable = true)\n",
      " |-- Flight Number: integer (nullable = true)\n",
      " |-- Destination Airport: string (nullable = true)\n",
      " |-- Actual elapsed time (Minutes): integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Table(name='dallas_electors', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='dallas_votes', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='flight', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='flights', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='flights_2014', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='flights_2015', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='flights_2017', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='flights_2018', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='people', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the Parquet file into flights_df\n",
    "file_path = 'output-files/AA_DFW_ALL.parquet'\n",
    "flights_df = spark.read.parquet(file_path)\n",
    "flights_df.printSchema()\n",
    "\n",
    "# Register the temp table\n",
    "flights_df.createOrReplaceTempView('flights')\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "285d08d9-b4ca-4b8e-b27d-3a7269920194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Date (MM/DD/YYYY)</th><th>Flight Number</th><th>Destination Airport</th><th>Actual elapsed time (Minutes)</th></tr>\n",
       "<tr><td>2017-01-01</td><td>5</td><td>HNL</td><td>537</td></tr>\n",
       "<tr><td>2017-01-01</td><td>7</td><td>OGG</td><td>498</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------------+-------------+-------------------+-----------------------------+\n",
       "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
       "+-----------------+-------------+-------------------+-----------------------------+\n",
       "|       2017-01-01|            5|                HNL|                          537|\n",
       "|       2017-01-01|            7|                OGG|                          498|\n",
       "+-----------------+-------------+-------------------+-----------------------------+"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_df.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8f65938-edc4-42a5-aec4-021a99fde793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(avg(Actual elapsed time (Minutes))=151.68865806809941)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run a SQL query of the average flight duration\n",
    "avg_duration = spark.sql('SELECT avg(`Actual elapsed time (Minutes)`) from flights')\n",
    "avg_duration.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fcfd3183-68e9-4f4a-b7ad-dafe5a393078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average flight time is: 151\n"
     ]
    }
   ],
   "source": [
    "print('The average flight time is: %d' % avg_duration.collect()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84744cb-27ee-40bb-a5c3-a8b24d7342c0",
   "metadata": {},
   "source": [
    "## Close session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a35b740-9e99-4e09-8ccc-81e0ea59e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382aa0d6-d5f7-4180-a803-f58b165acd30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
